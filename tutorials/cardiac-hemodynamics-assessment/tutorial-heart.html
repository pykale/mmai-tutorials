
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Cardiothoracic Abnormality Assessment &#8212; PyKale</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/exercise.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'tutorials/cardiac-hemodynamics-assessment/tutorial-heart';</script>
    <link rel="icon" href="../../_static/icon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Extension Tasks" href="extend-reading/extension-tasks.html" />
    <link rel="prev" title="Helper Functions" href="../brain-disorder-diagnosis/extend-reading/helper-functions.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../overview.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/embc_logo.png" class="logo__image only-light" alt="PyKale - Home"/>
    <script>document.write(`<img src="../../_static/embc_logo.png" class="logo__image only-dark" alt="PyKale - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../overview.html">
                    Overview
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Workshop</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../workshop/intro.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../workshop/schedule.html">Program</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../setup-config/tutorial-0.html">Setup &amp; Configuration</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../brain-disorder-diagnosis/tutorial-brain.html">Brain Disorder Diagnosis</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../brain-disorder-diagnosis/extend-reading/extension-tasks.html">Extension Tasks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../brain-disorder-diagnosis/extend-reading/data-config.html">Data &amp; Config (optional)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../brain-disorder-diagnosis/extend-reading/helper-functions.html">Helper Functions (optional)</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="current reference internal" href="#">Cardiothoracic Abnormality Assessment</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="extend-reading/extension-tasks.html">Extension Tasks</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../drug-target-interaction/tutorial-drug.html">Drug–Target Interaction Prediction</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../multiomics-cancer-classification/tutorial-cancer.html">Multiomics Cancer Classification</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../multiomics-cancer-classification/extend-reading/extension-tasks.html">Extension Tasks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../multiomics-cancer-classification/extend-reading/data.html">Data (optional)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../multiomics-cancer-classification/extend-reading/helper-functions.html">Helper Functions &amp; Model Details (optional)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../multiomics-cancer-classification/extend-reading/interpretation-study.html">Interpretation Study (optional)</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/pykale/mmai-tutorials/blob/main/tutorials/cardiac-hemodynamics-assessment/tutorial-heart.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/pykale/mmai-tutorials" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/pykale/mmai-tutorials/issues/new?title=Issue%20on%20page%20%2Ftutorials/cardiac-hemodynamics-assessment/tutorial-heart.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/tutorials/cardiac-hemodynamics-assessment/tutorial-heart.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Cardiothoracic Abnormality Assessment</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-0-environment-preparation">Step 0: Environment Preparation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#setup-google-colab-runtime-type">Setup Google Colab runtime type</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#package-installation">Package installation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#setup">Setup</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#configuration">Configuration</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pre-training-configuration">Pre-training Configuration</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuning-configuration">Fine-tuning Configuration</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-data-loading-and-preparation">Step 1: Data Loading and Preparation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pre-training-data-loading">Pre-training Data Loading</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuning-data-loading">Fine-tuning Data Loading</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-model-definition">Step 2: Model Definition</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#embed">Embed</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#signal-encoder">Signal Encoder</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#image-encoder">Image Encoder</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-fusion">Feature Fusion</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predict">Predict</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#reconstruction-pre-training">Reconstruction (Pre-training)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-fine-tuning">Classification (Fine-tuning)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-model-training">Step 3: Model Training</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multimodal-pretraining">Multimodal Pretraining</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multimodal-fine-tuning">Multimodal Fine-tuning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-evaluation">Step 4: Evaluation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-interpretation">Step 5: Interpretation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="cardiothoracic-abnormality-assessment">
<h1>Cardiothoracic Abnormality Assessment<a class="headerlink" href="#cardiothoracic-abnormality-assessment" title="Link to this heading">#</a></h1>
<p><img alt="" src="https://github.com/pykale/mmai-tutorials/blob/main/tutorials/cardiac-hemodynamics-assessment/images/embc_heart_tutorial_fig.png?raw=1" /></p>
<p>In this tutorial, we will use <code class="docutils literal notranslate"><span class="pre">PyKale</span></code> [1] to pretrain, fine-tune, evaluate, and interpret deep learning models on two low-cost, non-invasive data modalities: <strong>Chest X-ray (CXR)</strong> and <strong>12-lead Electrocardiogram (ECG)</strong>, for classifying individuals as <strong>healthy</strong> or having a <strong>cardiothoracic abnormality</strong>.</p>
<p>We will work with a multimodal dataset derived from <a class="reference external" href="https://physionet.org/content/mimic-cxr/2.1.0/">MIMIC-CXR</a> [2,3] and <a class="reference external" href="https://physionet.org/content/mimic-iv-ecg/1.0/">MIMIC-IV-ECG</a> [4], which contains approximately 50,000 paired CXR–ECG samples.</p>
<p>This tutorial is based on the work of <a class="reference external" href="https://link.springer.com/chapter/10.1007/978-3-031-72378-0_28">Suvon et al. (MICCAI 2024)</a> [5], which proposed a tri-stream pretraining method using a <strong>Multimodal Variational Autoencoder (VAE)</strong> to learn both modality-specific and modality-shared representations for estimating <strong>Pulmonary Arterial Wedge Pressure (PAWP)</strong>—a key indicator of cardiac hemodynamics.</p>
<p>The multimodal approach used in this tutorial involves <strong>intermediate fusion</strong>, where imaging (CXR) and ECG features are combined at feature embedding level.</p>
<p>The main tasks of this tutorial are:</p>
<ul class="simple">
<li><p>Load CXR and ECG data.</p></li>
<li><p>Pretrain a multimodal <strong>CardioVAE</strong> model using ~49,000 CXR–ECG pairs via a tri-stream pretraining method</p></li>
<li><p>Fine-tune the pretrained CardioVAE model on a smaller subset (~1,000 paired samples) with binary labels: <strong>Healthy</strong> and <strong>Cardiothoracic Abnormality</strong></p></li>
<li><p>Evaluate the performance of the fine-tuned model</p></li>
<li><p>Interpret the trained CardioVAE model across both CXR and ECG modalities</p></li>
</ul>
<p><strong>Estimated runtime:</strong> Completing the tasks in this tutorial will take approximately 10 minutes (<strong>GPU is required</strong>).</p>
<section id="step-0-environment-preparation">
<h2>Step 0: Environment Preparation<a class="headerlink" href="#step-0-environment-preparation" title="Link to this heading">#</a></h2>
<section id="setup-google-colab-runtime-type">
<h3>Setup Google Colab runtime type<a class="headerlink" href="#setup-google-colab-runtime-type" title="Link to this heading">#</a></h3>
<p>To run this tutorial in Google Colab, you need to use GPU runtime type. Click on the <strong>“Runtime”</strong> option in the top-left menu, then select <strong>“Change runtime type”</strong> and choose <strong>T4 GPU</strong> as the hardware accelerator.</p>
</section>
<section id="package-installation">
<h3>Package installation<a class="headerlink" href="#package-installation" title="Link to this heading">#</a></h3>
<p>The main packages required (excluding <code class="docutils literal notranslate"><span class="pre">PyKale</span></code>) for this tutorial are:</p>
<ul class="simple">
<li><p><strong>wfdb</strong>: A toolkit for reading, writing, and processing physiological signal data, especially useful for ECG waveform analysis.</p></li>
<li><p><strong>yacs</strong>: A lightweight configuration management library that helps organize experimental settings in a structured, readable format.</p></li>
<li><p><strong>pytorch-lightning</strong>: A high-level framework built on PyTorch that simplifies training workflows, making code cleaner and easier to scale.</p></li>
<li><p><strong>tabulate</strong>: Used to print tabular data in a readable format, helpful for summarizing results or configuration parameters.</p></li>
</ul>
<p><strong>Estimated runtime:</strong> 4 minutes</p>
<div class="cell tag_hide-input tag_hide-output docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell source</p>
<p class="expanded admonition-title">Hide code cell source</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">capture</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">--</span><span class="n">quiet</span> \
    <span class="s2">&quot;pykale[example]@git+https://github.com/pykale/pykale@main&quot;</span> \
    <span class="n">torch</span><span class="o">-</span><span class="n">geometric</span><span class="o">==</span><span class="mf">2.6.0</span> <span class="n">torch_sparse</span> <span class="n">torch_scatter</span> \
    <span class="o">-</span><span class="n">f</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">data</span><span class="o">.</span><span class="n">pyg</span><span class="o">.</span><span class="n">org</span><span class="o">/</span><span class="n">whl</span><span class="o">/</span><span class="n">torch</span><span class="o">-</span><span class="mf">2.6.0</span><span class="o">+</span><span class="n">cu124</span><span class="o">.</span><span class="n">html</span>
</pre></div>
</div>
</div>
</details>
</div>
</section>
<section id="setup">
<h3>Setup<a class="headerlink" href="#setup" title="Link to this heading">#</a></h3>
<p>As a starting point, we will mount Google Drive in Colab so that the data can be accessed directly. You might be prompted to grant permission to access your Google account—please proceed with the authorisation when asked.</p>
<div class="cell tag_hide-input tag_hide-output docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell source</p>
<p class="expanded admonition-title">Hide code cell source</p>
</summary>
<div class="cell_input above-output-prompt docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Connect with your google drive for data and model loading</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">google.colab</span><span class="w"> </span><span class="kn">import</span> <span class="n">drive</span>

<span class="n">drive</span><span class="o">.</span><span class="n">mount</span><span class="p">(</span><span class="s2">&quot;/content/drive&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<details class="admonition hide below-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell output</p>
<p class="expanded admonition-title">Hide code cell output</p>
</summary>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Mounted at /content/drive
</pre></div>
</div>
</div>
</details>
</div>
<p>Next, we will install the required packages and load a set of helper functions to support the tutorial workflow. To keep the output clean and focused on interpretation, we also suppress unnecessary warnings.</p>
<p><strong>Estimated runtime:</strong> 25 seconds</p>
<div class="cell tag_hide-input tag_hide-output docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell source</p>
<p class="expanded admonition-title">Hide code cell source</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">site</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>


<span class="c1"># Disable warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;PYTHONWARNINGS&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;ignore&quot;</span>

<span class="c1"># Suppress PyTorch Lightning logs</span>
<span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">&quot;pytorch_lightning&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">ERROR</span><span class="p">)</span>
<span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">&quot;pytorch_lightning.utilities.rank_zero&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">ERROR</span><span class="p">)</span>
<span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">&quot;pytorch_lightning.accelerators.cuda&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">ERROR</span><span class="p">)</span>

<span class="k">if</span> <span class="s2">&quot;google.colab&quot;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">get_ipython</span><span class="p">()):</span>
    <span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">site</span><span class="o">.</span><span class="n">getusersitepackages</span><span class="p">())</span>
    <span class="err">!</span><span class="n">git</span> <span class="n">clone</span> <span class="o">-</span><span class="n">q</span> <span class="o">--</span><span class="n">single</span><span class="o">-</span><span class="n">branch</span> <span class="o">-</span><span class="n">b</span> <span class="n">main</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">pykale</span><span class="o">/</span><span class="n">mmai</span><span class="o">-</span><span class="n">tutorials</span>
    <span class="o">%</span><span class="n">cp</span> <span class="o">-</span><span class="n">r</span> <span class="o">/</span><span class="n">content</span><span class="o">/</span><span class="n">mmai</span><span class="o">-</span><span class="n">tutorials</span><span class="o">/</span><span class="n">tutorials</span><span class="o">/</span><span class="n">cardiac</span><span class="o">-</span><span class="n">hemodynamics</span><span class="o">-</span><span class="n">assessment</span><span class="o">/*</span> <span class="o">/</span><span class="n">content</span><span class="o">/</span>
    <span class="o">%</span><span class="n">rm</span> <span class="o">-</span><span class="n">r</span> <span class="o">/</span><span class="n">content</span><span class="o">/</span><span class="n">mmai</span><span class="o">-</span><span class="n">tutorials</span>
</pre></div>
</div>
</div>
</details>
</div>
</section>
<section id="configuration">
<h3>Configuration<a class="headerlink" href="#configuration" title="Link to this heading">#</a></h3>
<p>To maintain a clean and modular notebook design, <strong>CardioVAE</strong> uses dedicated configuration files for both pre-training and fine-tuning. This setup ensures a clear separation between code and experimental settings, enhancing reproducibility and flexibility across different tasks and datasets.</p>
<p>Configuration parameters can be overridden using external YAML files (e.g., <code class="docutils literal notranslate"><span class="pre">experiments/pretraining_base.yml</span></code>, <code class="docutils literal notranslate"><span class="pre">experiments/finetune_base.yml</span></code>).</p>
<section id="pre-training-configuration">
<h4>Pre-training Configuration<a class="headerlink" href="#pre-training-configuration" title="Link to this heading">#</a></h4>
<p>Default settings for the pre-training stage are defined in <code class="docutils literal notranslate"><span class="pre">config_pretrain.py</span></code>. These include data paths, model architecture, and optimizer parameters.
This modular structure allows easy experiment tracking and customisation by simply editing the associated <code class="docutils literal notranslate"><span class="pre">.yml</span></code> file.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">config_pretrain</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_cfg_defaults</span>

<span class="n">cfg_PT</span> <span class="o">=</span> <span class="n">get_cfg_defaults</span><span class="p">()</span>
<span class="n">cfg_PT</span><span class="o">.</span><span class="n">merge_from_file</span><span class="p">(</span><span class="s2">&quot;configs/pretraining_base.yml&quot;</span><span class="p">)</span>

<span class="c1"># ------ Hyperparameters to play with -----</span>
<span class="n">cfg_PT</span><span class="o">.</span><span class="n">MODEL</span><span class="o">.</span><span class="n">LATENT_DIM</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">cfg_PT</span><span class="o">.</span><span class="n">TRAIN</span><span class="o">.</span><span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">cfg_PT</span><span class="o">.</span><span class="n">TRAIN</span><span class="o">.</span><span class="n">LAMBDA_IMAGE</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">cfg_PT</span><span class="o">.</span><span class="n">TRAIN</span><span class="o">.</span><span class="n">LAMBDA_SIGNAL</span> <span class="o">=</span> <span class="mi">10</span>
<span class="c1"># User can change this to try different batch size.</span>
<span class="n">cfg_PT</span><span class="o">.</span><span class="n">DATA</span><span class="o">.</span><span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">128</span>

<span class="nb">print</span><span class="p">(</span><span class="n">cfg_PT</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>DATA:
  BATCH_SIZE: 128
  CXR_PATH: /content/drive/MyDrive/EMBC_workshop_data/cxr_features_tensor_1000.pt
  ECG_PATH: /content/drive/MyDrive/EMBC_workshop_data/ecg_features_tensor_1000.pt
  NUM_WORKERS: 2
MODEL:
  INPUT_DIM_CXR: 1
  INPUT_DIM_ECG: 60000
  LATENT_DIM: 128
  NUM_LEADS: 12
TRAIN:
  ACCELERATOR: gpu
  DATA_DEVICE: cpu
  DEVICE: cuda
  DEVICES: 1
  EPOCHS: 1
  LAMBDA_IMAGE: 1
  LAMBDA_SIGNAL: 10
  LR: 0.001
  SAVE_PATH: cardioVAE.pth
  SCALE_FACTOR: 0.0001
  SEED: 123
</pre></div>
</div>
</div>
</div>
</section>
<section id="fine-tuning-configuration">
<h4>Fine-tuning Configuration<a class="headerlink" href="#fine-tuning-configuration" title="Link to this heading">#</a></h4>
<p>Fine-tuning parameters are managed in <code class="docutils literal notranslate"><span class="pre">config_finetune.py</span></code>. These include learning rate, loss weights, number of epochs, model checkpoint paths, and other task-specific options.
External YAML files like <code class="docutils literal notranslate"><span class="pre">experiments/finetune_base.yml</span></code> enable flexible adjustments for different downstream tasks or datasets.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">config_finetune</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_cfg_defaults</span>

<span class="n">cfg_FT</span> <span class="o">=</span> <span class="n">get_cfg_defaults</span><span class="p">()</span>
<span class="n">cfg_FT</span><span class="o">.</span><span class="n">merge_from_file</span><span class="p">(</span><span class="s2">&quot;configs/finetune_base.yml&quot;</span><span class="p">)</span>

<span class="c1"># ------ Hyperparameters to play with -----</span>
<span class="n">cfg_FT</span><span class="o">.</span><span class="n">FT</span><span class="o">.</span><span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">cfg_FT</span><span class="o">.</span><span class="n">FT</span><span class="o">.</span><span class="n">HIDDEN_DIM</span> <span class="o">=</span> <span class="mi">128</span>
<span class="c1"># User can change this to try different batch size.</span>
<span class="n">cfg_FT</span><span class="o">.</span><span class="n">DATA</span><span class="o">.</span><span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">32</span>

<span class="nb">print</span><span class="p">(</span><span class="n">cfg_FT</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>DATA:
  BATCH_SIZE: 32
  CSV_PATH: /content/drive/MyDrive/EMBC_workshop_data/chexpert_healthy_abnormality_subset.csv
  CXR_PATH: /content/drive/MyDrive/EMBC_workshop_data/cxr_features_tensor_last_1000.pt
  DATA_DEVICE: cpu
  ECG_PATH: /content/drive/MyDrive/EMBC_workshop_data/ecg_features_tensor_last_1000.pt
  NUM_WORKERS: 2
FT:
  ACCELERATOR: gpu
  CKPT_PATH: /content/drive/MyDrive/EMBC_workshop_data/CardioVAE.pth
  DEVICE: cuda
  DEVICES: 1
  EPOCHS: 10
  HIDDEN_DIM: 128
  KFOLDS: 5
  LR: 0.001
  NUM_CLASSES: 2
  SEED: 42
INTERPRET:
  CXR_THRESHOLD: 0.7
  ECG_THRESHOLD: 0.7
  SAMPLE_IDX: 101
  SAMPLING_RATE: 500
  ZOOM_RANGE: [3, 3.5]
MODEL:
  INPUT_DIM_ECG: 60000
  INPUT_IMAGE_CHANNELS: 1
  LATENT_DIM: 256
  NUM_LEADS: 12
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="step-1-data-loading-and-preparation">
<h2>Step 1: Data Loading and Preparation<a class="headerlink" href="#step-1-data-loading-and-preparation" title="Link to this heading">#</a></h2>
<p>This tutorial uses separate data pipelines for <strong>pre-training</strong> and <strong>fine-tuning</strong>, both based on paired chest X-ray (CXR) and ECG signal features. Each stage follows standard preprocessing steps—such as resizing, normalization, interpolation, and tensor conversion—tailored for resource-constrained environments like <strong>Google Colab</strong>.</p>
<p>PyKale API for Data preparation:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/pykale/pykale/blob/main/kale/loaddata/signal_access.py"><code class="docutils literal notranslate"><span class="pre">kale.loaddata.signal_access.load_ecg_from_folder</span></code></a> provides a convenient function for loading ECG waveform data stored in a directory structure. It supports automatic parsing and conversion of ECG signal files into PyTorch tensors, with options for standard preprocessing such as normalisation, resize, interpolation, and resampling. This enables streamlined integration with deep learning pipelines.</p></li>
<li><p><a class="reference external" href="https://github.com/pykale/pykale/blob/main/kale/loaddata/image_access.py"><code class="docutils literal notranslate"><span class="pre">kale.loaddata.image_access.load_images_from_dir</span></code></a> offers an easy-to-use utility for loading image datasets from directory hierarchies. It supports standard image formats and returns PyTorch tensors, performing essential preprocessing steps such as resizing and normalisation. This function is suitable for image classification, computer vision, and multimodal learning tasks.</p></li>
<li><p><a class="reference external" href="https://github.com/pykale/pykale/blob/main/kale/loaddata/signal_image_access.py"><code class="docutils literal notranslate"><span class="pre">kale.loaddata.signal_image_access.SignalImageDataset</span></code></a> defines a unified dataset class designed for paired signal (e.g., ECG) and image (e.g., CXR) modalities. It facilitates synchronized access to multi-source data, providing ready-to-use PyTorch datasets that can be directly utilised for multimodal training, evaluation, and downstream applications.</p></li>
</ul>
<p><strong>Note:</strong> Please create a shortcut to the following Google Drive folder in your <strong>MyDrive</strong>.
To create a shortcut:
(i) Click the link to open the Google Drive folder.
(ii) Click the folder name at the top to reveal a <strong>drop-down menu</strong>.
(iii) From the drop-down menu, select <strong>Organise &gt; Add shortcut</strong>.
(iv) A dialog box titled <em>“Add shortcut to ‘EMBC_workshop_data’”</em> will appear — click the <strong>“All locations”</strong> tab, then select <strong>“My Drive”</strong>.
(v) A shortcut to <strong>EMBC_workshop_data</strong> should now be visible at <a class="reference external" href="https://drive.google.com/drive/my-drive">https://drive.google.com/drive/my-drive</a>.</p>
<p><a class="reference external" href="https://drive.google.com/drive/folders/1N7-fMWsdK-tuB76SdC-GF1njYYGx0Z-i?usp=sharing">Google Drive Link</a></p>
<p>There’s no need to download the data manually. After mounting your Google Drive in the setup section, you will be able to directly access all datasets and pretrained models.</p>
<section id="pre-training-data-loading">
<h3>Pre-training Data Loading<a class="headerlink" href="#pre-training-data-loading" title="Link to this heading">#</a></h3>
<p>To accommodate the resource constraints of platforms like <strong>Google Colab</strong>, this tutorial uses a lightweight version of the dataset consisting of the <strong>first 1,000 preprocessed samples</strong> from the full 50K paired CXR and ECG dataset. This significantly reduces runtime and memory requirements, allowing for rapid experimentation without the overhead of full-scale data loading and transformation.</p>
<p>The complete preprocessing pipeline, implemented using the PyKale API, is provided for reference. Additionally, CSV files containing subject IDs for the full dataset are provided. Users interested in training on the complete 50K dataset can leverage the <strong>PyKale API</strong>, which supports direct loading of raw CXR and ECG features with integrated preprocessing.</p>
<p><strong>Note:</strong></p>
<ul class="simple">
<li><p>For ease of use in Colab, the full data loading functionality is <strong>commented out by default</strong>. It can be re-enabled for local or high-resource environments. To load the full 50K paired CXR-ECG data, you need to download the <a class="reference external" href="https://physionet.org/content/mimic-cxr/2.1.0/">MIMIC-CXR</a> and <a class="reference external" href="https://physionet.org/content/mimic-iv-ecg/1.0/">MIMIC-IV-ECG</a> datasets, then <strong>uncomment</strong> the optional code cell and run it.</p></li>
<li><p>To access the required files for dataloading, ensure that the shared folder <strong><code class="docutils literal notranslate"><span class="pre">EMBC_workshop_data</span></code></strong> is added as a <strong>shortcut to your Google Drive (under “My Drive”)</strong>.</p></li>
</ul>
<p><strong>Estimated runtime:</strong> 12 seconds</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># (OPTIONAL)</span>
<span class="c1"># from kale.loaddata.signal_access import load_ecg_from_folder</span>
<span class="c1"># from kale.loaddata.image_access import load_images_from_dir</span>

<span class="c1"># ecg_tensor = load_ecg_from_folder(&quot;/mimic-iv-ecg-diagnostic-electrocardiogram-matched-subset-1.0/&quot;, &quot;mimic_ecg_50K.csv&quot;)</span>
<span class="c1"># cxr_tensor = load_images_from_dir(&quot;/physionet.org/files/mimic-cxr-jpg/2.0.0/&quot;, &quot;mimic_cxr_50K.csv&quot;)</span>

<span class="c1"># train_dataset_PT, val_dataset_PT = SignalImageDataset.prepare_data_loaders( ecg_tensor, cxr_tensor)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">kale.loaddata.signal_image_access</span><span class="w"> </span><span class="kn">import</span> <span class="n">SignalImageDataset</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">kale.utils.seed</span><span class="w"> </span><span class="kn">import</span> <span class="n">set_seed</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">random</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="n">set_seed</span><span class="p">(</span><span class="n">cfg_PT</span><span class="o">.</span><span class="n">TRAIN</span><span class="o">.</span><span class="n">SEED</span><span class="p">)</span>

<span class="n">ecg_tensor_PT</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">cfg_PT</span><span class="o">.</span><span class="n">DATA</span><span class="o">.</span><span class="n">ECG_PATH</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">cfg_PT</span><span class="o">.</span><span class="n">TRAIN</span><span class="o">.</span><span class="n">DATA_DEVICE</span><span class="p">)</span>
<span class="n">cxr_tensor_PT</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">cfg_PT</span><span class="o">.</span><span class="n">DATA</span><span class="o">.</span><span class="n">CXR_PATH</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">cfg_PT</span><span class="o">.</span><span class="n">TRAIN</span><span class="o">.</span><span class="n">DATA_DEVICE</span><span class="p">)</span>

<span class="n">train_dataset_PT</span><span class="p">,</span> <span class="n">val_dataset_PT</span> <span class="o">=</span> <span class="n">SignalImageDataset</span><span class="o">.</span><span class="n">prepare_data_loaders</span><span class="p">(</span>
    <span class="n">ecg_tensor_PT</span><span class="p">,</span> <span class="n">cxr_tensor_PT</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="fine-tuning-data-loading">
<h3>Fine-tuning Data Loading<a class="headerlink" href="#fine-tuning-data-loading" title="Link to this heading">#</a></h3>
<p>For the fine-tuning stage, we use the <strong>last 1,000 paired CXR and ECG samples</strong> from the full 50K dataset derived from <strong>MIMIC-CXR</strong> and <strong>MIMIC-IV-ECG</strong>. Corresponding disease labels are obtained from MIMIC-CXR, which includes 12 cardiothoracic abnormality classes along with a “No Finding” label representing healthy cases.</p>
<p>To formulate a binary classification task, all abnormality classes are grouped into a single <strong>Cardiothoracic Abnormality</strong> category, while the “No Finding” label is treated as <strong>Healthy</strong>. The resulting label mapping is as follows:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">0</span></code>: <strong>Healthy</strong></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">1</span></code>: <strong>Cardiothoracic Abnormality</strong></p></li>
</ul>
<p>This fine-tuning subset is explicitly selected to ensure no overlap with the samples used during pre-training, thereby simulating a realistic downstream evaluation setting.</p>
<p>Unlike the fine-tuning strategy reported in <em>Suvon et al., MICCAI 2024</em>, which relied on a private in-house dataset, this approach is fully reproducible using publicly available data from MIMIC-CXR and MIMIC-IV-ECG.</p>
<p><strong>Estimated runtime:</strong> 10 seconds</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">TensorDataset</span><span class="p">,</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">random_split</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="c1"># Set seed for reproducibility</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">cfg_FT</span><span class="o">.</span><span class="n">FT</span><span class="o">.</span><span class="n">SEED</span><span class="p">)</span>

<span class="c1"># Load data</span>
<span class="n">ecg_tensor_FT</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">cfg_FT</span><span class="o">.</span><span class="n">DATA</span><span class="o">.</span><span class="n">ECG_PATH</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">cfg_FT</span><span class="o">.</span><span class="n">DATA</span><span class="o">.</span><span class="n">DATA_DEVICE</span><span class="p">)</span>
<span class="n">cxr_tensor_FT</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">cfg_FT</span><span class="o">.</span><span class="n">DATA</span><span class="o">.</span><span class="n">CXR_PATH</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">cfg_FT</span><span class="o">.</span><span class="n">DATA</span><span class="o">.</span><span class="n">DATA_DEVICE</span><span class="p">)</span>
<span class="n">label_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">cfg_FT</span><span class="o">.</span><span class="n">DATA</span><span class="o">.</span><span class="n">CSV_PATH</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">label_df</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>

<span class="c1"># Combine tensors into a single dataset</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">cxr_tensor_FT</span><span class="p">,</span> <span class="n">ecg_tensor_FT</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

<span class="c1"># Split into train/val</span>
<span class="n">val_ratio</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">num_samples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
<span class="n">num_val</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">val_ratio</span> <span class="o">*</span> <span class="n">num_samples</span><span class="p">)</span>
<span class="n">num_train</span> <span class="o">=</span> <span class="n">num_samples</span> <span class="o">-</span> <span class="n">num_val</span>

<span class="n">train_dataset_FT</span><span class="p">,</span> <span class="n">val_dataset_FT</span> <span class="o">=</span> <span class="n">random_split</span><span class="p">(</span>
    <span class="n">dataset</span><span class="p">,</span>
    <span class="p">[</span><span class="n">num_train</span><span class="p">,</span> <span class="n">num_val</span><span class="p">],</span>
    <span class="n">generator</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">cfg_FT</span><span class="o">.</span><span class="n">FT</span><span class="o">.</span><span class="n">SEED</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># DataLoaders</span>
<span class="n">train_loader_FT</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">train_dataset_FT</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">cfg_FT</span><span class="o">.</span><span class="n">DATA</span><span class="o">.</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="o">=</span><span class="n">cfg_FT</span><span class="o">.</span><span class="n">DATA</span><span class="o">.</span><span class="n">NUM_WORKERS</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">val_loader_FT</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">val_dataset_FT</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">cfg_FT</span><span class="o">.</span><span class="n">DATA</span><span class="o">.</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="o">=</span><span class="n">cfg_FT</span><span class="o">.</span><span class="n">DATA</span><span class="o">.</span><span class="n">NUM_WORKERS</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="step-2-model-definition">
<h2>Step 2: Model Definition<a class="headerlink" href="#step-2-model-definition" title="Link to this heading">#</a></h2>
<p>We use the <strong>PyKale</strong> library to implement a modular multimodal variational autoencoder (VAE) for learning joint representations from <strong>ECG</strong> and <strong>CXR</strong> data. The architecture includes modality-specific encoders and decoders, a PoE-based fusion mechanism, and task-specific heads for reconstruction and classification.</p>
<section id="embed">
<h3>Embed<a class="headerlink" href="#embed" title="Link to this heading">#</a></h3>
<p>The embedding module is composed of independent encoders for each modality and a fusion mechanism to obtain a shared latent representation.</p>
<section id="signal-encoder">
<h4>Signal Encoder<a class="headerlink" href="#signal-encoder" title="Link to this heading">#</a></h4>
<p>The ECG signal pathway uses <code class="docutils literal notranslate"><span class="pre">SignalVAEEncoder</span></code> from <a class="reference external" href="https://github.com/pykale/pykale/blob/main/kale/embed/signal_cnn.py"><code class="docutils literal notranslate"><span class="pre">kale.embed.signal_cnn</span></code></a>.
This encoder captures high-level temporal features from preprocessed ECG waveforms and maps them to a latent space suitable for downstream fusion and representation learning.</p>
</section>
<section id="image-encoder">
<h4>Image Encoder<a class="headerlink" href="#image-encoder" title="Link to this heading">#</a></h4>
<p>The image pathway uses <code class="docutils literal notranslate"><span class="pre">ImageVAEEncoder</span></code> from <a class="reference external" href="https://github.com/pykale/pykale/blob/main/kale/embed/image_cnn.py"><code class="docutils literal notranslate"><span class="pre">kale.embed.image_cnn</span></code></a>.
This encoder captures high-level spatial features from preprocessed CXR’s and maps them to a latent space suitable for downstream fusion and representation learning.</p>
</section>
<section id="feature-fusion">
<h4>Feature Fusion<a class="headerlink" href="#feature-fusion" title="Link to this heading">#</a></h4>
<p>Encoded modality-specific features are combined using a <strong>Product-of-Experts (PoE)</strong> approach, implemented in <a class="reference external" href="https://github.com/pykale/pykale/blob/main/kale/embed/feature_fusion.py"><code class="docutils literal notranslate"><span class="pre">kale.embed.feature_fusion</span></code></a>.
The PoE fusion computes a joint posterior over the latent space by aggregating information from each modality, enabling coherent multimodal representation.</p>
</section>
</section>
<section id="predict">
<h3>Predict<a class="headerlink" href="#predict" title="Link to this heading">#</a></h3>
<section id="reconstruction-pre-training">
<h4>Reconstruction (Pre-training)<a class="headerlink" href="#reconstruction-pre-training" title="Link to this heading">#</a></h4>
<p>During pre-training, the model reconstructs both modalities using decoders from the shared latent representation:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">ImageVAEDecoder</span></code> from <a class="reference external" href="https://github.com/pykale/pykale/blob/main/kale/predict/decode.py"><code class="docutils literal notranslate"><span class="pre">kale.predict.decode</span></code></a> for CXR reconstruction</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">SignalVAEDecoder</span></code> from <a class="reference external" href="https://github.com/pykale/pykale/blob/main/kale/predict/decode.py"><code class="docutils literal notranslate"><span class="pre">kale.predict.decode</span></code></a> for ECG waveform reconstruction</p></li>
</ul>
<p>The model is trained to minimise the <strong>Evidence Lower Bound (ELBO)</strong>, encouraging informative and disentangled latent representations.</p>
</section>
<section id="classification-fine-tuning">
<h4>Classification (Fine-tuning)<a class="headerlink" href="#classification-fine-tuning" title="Link to this heading">#</a></h4>
<p>For downstream classification tasks, we reuse the pretrained encoders as feature extractors.
The <code class="docutils literal notranslate"><span class="pre">SignalImageFineTuningClassifier</span></code> from <a class="reference external" href="https://github.com/pykale/pykale/blob/main/kale/predict/decode.py"><code class="docutils literal notranslate"><span class="pre">kale.predict.decode</span></code></a> adds a lightweight classification head on top of the shared latent space for supervised learning.
This setup is optimised for clinical prediction tasks, such as binary or multi-label disease classification.</p>
</section>
</section>
</section>
<section id="step-3-model-training">
<h2>Step 3: Model Training<a class="headerlink" href="#step-3-model-training" title="Link to this heading">#</a></h2>
<section id="multimodal-pretraining">
<h3>Multimodal Pretraining<a class="headerlink" href="#multimodal-pretraining" title="Link to this heading">#</a></h3>
<p>We pretraind a CardioVAE model using the <code class="docutils literal notranslate"><span class="pre">SignalImageVAE</span></code> class from <strong>PyKale</strong> to jointly model paired CXR and ECG data. The goal is to learn <strong>shared and modality-specific representations</strong> in an <strong>unsupervised</strong> manner via reconstruction.</p>
<p>We instantiate <code class="docutils literal notranslate"><span class="pre">SignalImageVAE</span></code> from <a class="reference external" href="https://github.com/pykale/pykale/blob/main/kale/embed/multimodal_encoder.py"><code class="docutils literal notranslate"><span class="pre">kale.embed.multimodal_encoder</span></code></a>, which includes:</p>
<ul class="simple">
<li><p>A <strong>signal encoder-decoder</strong> built on <code class="docutils literal notranslate"><span class="pre">SignalVAEEncoder</span></code> for ECG waveforms</p></li>
<li><p>An <strong>image encoder-decoder</strong> built on <code class="docutils literal notranslate"><span class="pre">ImageVAEEncoder</span></code> for CXR images</p></li>
<li><p>A <strong>Product-of-Experts (PoE)</strong> fusion module for combining modality-specific latent vectors into a shared latent representation</p></li>
</ul>
<p>To pretrain, we use <code class="docutils literal notranslate"><span class="pre">SignalImageTriStreamVAETrainer</span></code> from <a class="reference external" href="https://github.com/pykale/pykale/blob/main/kale/pipeline/multimodal_trainer.py"><code class="docutils literal notranslate"><span class="pre">kale.pipeline.multimodal_trainer</span></code></a> to:</p>
<ul class="simple">
<li><p>Perform <strong>joint and single-modality reconstructions</strong></p></li>
<li><p>Optimise the <strong>ELBO loss</strong>, balancing image and signal modalities</p></li>
<li><p>Manage logging, and reconstruction-based validation</p></li>
</ul>
<p><strong>Estimated runtime:</strong> 2 minutes with 1 epoch</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pytorch_lightning</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pl</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">kale.pipeline.multimodal_trainer</span><span class="w"> </span><span class="kn">import</span> <span class="n">SignalImageTriStreamVAETrainer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">kale.embed.multimodal_encoder</span><span class="w"> </span><span class="kn">import</span> <span class="n">SignalImageVAE</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SignalImageVAE</span><span class="p">(</span>
    <span class="n">image_input_channels</span><span class="o">=</span><span class="n">cfg_PT</span><span class="o">.</span><span class="n">MODEL</span><span class="o">.</span><span class="n">INPUT_DIM_CXR</span><span class="p">,</span>
    <span class="n">signal_input_dim</span><span class="o">=</span><span class="n">cfg_PT</span><span class="o">.</span><span class="n">MODEL</span><span class="o">.</span><span class="n">INPUT_DIM_ECG</span><span class="p">,</span>
    <span class="n">latent_dim</span><span class="o">=</span><span class="n">cfg_PT</span><span class="o">.</span><span class="n">MODEL</span><span class="o">.</span><span class="n">LATENT_DIM</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># PyKale trainer instance (all from config)</span>
<span class="n">pl_trainer</span> <span class="o">=</span> <span class="n">SignalImageTriStreamVAETrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset_PT</span><span class="p">,</span>
    <span class="n">val_dataset</span><span class="o">=</span><span class="n">val_dataset_PT</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">cfg_PT</span><span class="o">.</span><span class="n">DATA</span><span class="o">.</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="o">=</span><span class="n">cfg_PT</span><span class="o">.</span><span class="n">DATA</span><span class="o">.</span><span class="n">NUM_WORKERS</span><span class="p">,</span>
    <span class="n">lambda_image</span><span class="o">=</span><span class="n">cfg_PT</span><span class="o">.</span><span class="n">TRAIN</span><span class="o">.</span><span class="n">LAMBDA_IMAGE</span><span class="p">,</span>
    <span class="n">lambda_signal</span><span class="o">=</span><span class="n">cfg_PT</span><span class="o">.</span><span class="n">TRAIN</span><span class="o">.</span><span class="n">LAMBDA_SIGNAL</span><span class="p">,</span>
    <span class="n">lr</span><span class="o">=</span><span class="n">cfg_PT</span><span class="o">.</span><span class="n">TRAIN</span><span class="o">.</span><span class="n">LR</span><span class="p">,</span>
    <span class="n">annealing_epochs</span><span class="o">=</span><span class="n">cfg_PT</span><span class="o">.</span><span class="n">TRAIN</span><span class="o">.</span><span class="n">EPOCHS</span><span class="p">,</span>
    <span class="n">scale_factor</span><span class="o">=</span><span class="n">cfg_PT</span><span class="o">.</span><span class="n">TRAIN</span><span class="o">.</span><span class="n">SCALE_FACTOR</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span>
    <span class="n">max_epochs</span><span class="o">=</span><span class="n">cfg_PT</span><span class="o">.</span><span class="n">TRAIN</span><span class="o">.</span><span class="n">EPOCHS</span><span class="p">,</span>
    <span class="n">accelerator</span><span class="o">=</span><span class="n">cfg_PT</span><span class="o">.</span><span class="n">TRAIN</span><span class="o">.</span><span class="n">ACCELERATOR</span><span class="p">,</span>
    <span class="n">devices</span><span class="o">=</span><span class="n">cfg_PT</span><span class="o">.</span><span class="n">TRAIN</span><span class="o">.</span><span class="n">DEVICES</span><span class="p">,</span>
    <span class="n">log_every_n_steps</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">pl_trainer</span><span class="p">)</span>

<span class="c1"># Save model state dict</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">cfg_PT</span><span class="o">.</span><span class="n">TRAIN</span><span class="o">.</span><span class="n">SAVE_PATH</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Saved model state dictionary to &#39;</span><span class="si">{</span><span class="n">cfg_PT</span><span class="o">.</span><span class="n">TRAIN</span><span class="o">.</span><span class="n">SAVE_PATH</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>INFO:pytorch_lightning.callbacks.model_summary:
  | Name  | Type           | Params | Mode 
-------------------------------------------------
0 | model | SignalImageVAE | 204 M  | train
-------------------------------------------------
204 M     Trainable params
0         Non-trainable params
204 M     Total params
816.722   Total estimated model params size (MB)
32        Modules in train mode
0         Modules in eval mode
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "c2bf14b098074817bec6d483f8c841fb", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "19529492df8141189beecb6d8b17098d", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "87f89e0f723d463ab46f3c5212f0ea40", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Saved model state dictionary to &#39;cardioVAE.pth&#39;
</pre></div>
</div>
</div>
</div>
</section>
<section id="multimodal-fine-tuning">
<h3>Multimodal Fine-tuning<a class="headerlink" href="#multimodal-fine-tuning" title="Link to this heading">#</a></h3>
<p>For downstream classification, we fine-tune a shallow classifier on top of a <strong>pretrained CardioVAE encoder</strong>. The encoder is loaded from <code class="docutils literal notranslate"><span class="pre">SignalImageVAE</span></code>, pretrained with reconstruction loss, and used as a fixed or partially trainable <strong>feature extractor</strong>.</p>
<p>We use <code class="docutils literal notranslate"><span class="pre">SignalImageFineTuningTrainer</span></code> from <a class="reference external" href="https://github.com/pykale/pykale/blob/main/kale/pipeline/multimodal_trainer.py"><code class="docutils literal notranslate"><span class="pre">kale.pipeline.multimodal_trainer</span></code></a>, which wraps:</p>
<ul class="simple">
<li><p>A <strong>pretrained CardioVAE model</strong></p></li>
<li><p>A <strong>SignalImageFineTuningClassifier</strong> (two-layer fully connected classifier)</p></li>
<li><p>A <strong>training step</strong> that supports standard supervised learning with cross-entropy loss</p></li>
</ul>
<p><strong>Estimated runtime:</strong> 2 minute with 10 epoch</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pytorch_lightning</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pl</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">kale.embed.multimodal_encoder</span><span class="w"> </span><span class="kn">import</span> <span class="n">SignalImageVAE</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">kale.pipeline.multimodal_trainer</span><span class="w"> </span><span class="kn">import</span> <span class="n">SignalImageFineTuningTrainer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">kale.utils.remap_model_parameters</span><span class="w"> </span><span class="kn">import</span> <span class="n">remap_state_dict_keys</span>

<span class="c1"># Load and remap checkpoint</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">cfg_FT</span><span class="o">.</span><span class="n">FT</span><span class="o">.</span><span class="n">CKPT_PATH</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">cfg_FT</span><span class="o">.</span><span class="n">FT</span><span class="o">.</span><span class="n">DEVICE</span><span class="p">)</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="n">remap_state_dict_keys</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>

<span class="n">pretrained_mvae</span> <span class="o">=</span> <span class="n">SignalImageVAE</span><span class="p">(</span>
    <span class="n">image_input_channels</span><span class="o">=</span><span class="n">cfg_FT</span><span class="o">.</span><span class="n">MODEL</span><span class="o">.</span><span class="n">INPUT_IMAGE_CHANNELS</span><span class="p">,</span>
    <span class="n">signal_input_dim</span><span class="o">=</span><span class="n">cfg_FT</span><span class="o">.</span><span class="n">MODEL</span><span class="o">.</span><span class="n">INPUT_DIM_ECG</span><span class="p">,</span>
    <span class="n">latent_dim</span><span class="o">=</span><span class="n">cfg_FT</span><span class="o">.</span><span class="n">MODEL</span><span class="o">.</span><span class="n">LATENT_DIM</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">pretrained_mvae</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">pretrained_mvae</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cfg_FT</span><span class="o">.</span><span class="n">FT</span><span class="o">.</span><span class="n">DEVICE</span><span class="p">)</span>
<span class="n">pretrained_mvae</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="n">model_pl</span> <span class="o">=</span> <span class="n">SignalImageFineTuningTrainer</span><span class="p">(</span>
    <span class="n">pretrained_model</span><span class="o">=</span><span class="n">pretrained_mvae</span><span class="p">,</span>
    <span class="n">num_classes</span><span class="o">=</span><span class="n">cfg_FT</span><span class="o">.</span><span class="n">FT</span><span class="o">.</span><span class="n">NUM_CLASSES</span><span class="p">,</span>
    <span class="n">lr</span><span class="o">=</span><span class="n">cfg_FT</span><span class="o">.</span><span class="n">FT</span><span class="o">.</span><span class="n">LR</span><span class="p">,</span>
    <span class="n">hidden_dim</span><span class="o">=</span><span class="n">cfg_FT</span><span class="o">.</span><span class="n">FT</span><span class="o">.</span><span class="n">HIDDEN_DIM</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span>
    <span class="n">max_epochs</span><span class="o">=</span><span class="n">cfg_FT</span><span class="o">.</span><span class="n">FT</span><span class="o">.</span><span class="n">EPOCHS</span><span class="p">,</span>
    <span class="n">accelerator</span><span class="o">=</span><span class="n">cfg_FT</span><span class="o">.</span><span class="n">FT</span><span class="o">.</span><span class="n">ACCELERATOR</span><span class="p">,</span>
    <span class="n">devices</span><span class="o">=</span><span class="n">cfg_FT</span><span class="o">.</span><span class="n">FT</span><span class="o">.</span><span class="n">DEVICES</span><span class="p">,</span>
    <span class="n">log_every_n_steps</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">enable_checkpointing</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">logger</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model_pl</span><span class="p">,</span> <span class="n">train_dataloaders</span><span class="o">=</span><span class="n">train_loader_FT</span><span class="p">,</span> <span class="n">val_dataloaders</span><span class="o">=</span><span class="n">val_loader_FT</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>INFO:pytorch_lightning.callbacks.model_summary:
  | Name         | Type                            | Params | Mode 
-------------------------------------------------------------------------
0 | model        | SignalImageFineTuningClassifier | 271 M  | train
1 | val_accuracy | BinaryAccuracy                  | 0      | train
2 | val_auc      | BinaryAUROC                     | 0      | train
3 | val_mcc      | BinaryMatthewsCorrCoef          | 0      | train
-------------------------------------------------------------------------
65.9 K    Trainable params
271 M     Non-trainable params
271 M     Total params
1,086.193 Total estimated model params size (MB)
9         Modules in train mode
16        Modules in eval mode
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "c9818cc16b964897ad3f75206f978fa5", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "dc1963c152564725a440223ac3af66ff", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "c842510b30594cffbaec92680577e6c9", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "eb27cce888e649cb9273f14fa2b94fe4", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "5f09c430b3aa4cf29358bf448528beec", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "eea70264dbb04eb4b7886df6a870b773", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "7f357b8a2d5b4b5889e568308a8381d7", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "a37e20375c0f479d860a5dfefc08b628", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "912dd0effc5e49c6aae750d10b8338ad", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "a7ed29faf7bd427a9ae9a618fc1e55a9", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "480a539c264b4038898e0fbfd9be6100", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "fbb7afeca1a4404eab0e0b86e55bab8a", "version_major": 2, "version_minor": 0}</script></div>
</div>
</section>
</section>
<section id="step-4-evaluation">
<h2>Step 4: Evaluation<a class="headerlink" href="#step-4-evaluation" title="Link to this heading">#</a></h2>
<p>After training, we extract key validation metrics using PyTorch Lightning’s built-in <code class="docutils literal notranslate"><span class="pre">callback_metrics</span></code>. These metrics provide a quantitative summary of model performance on the validation set.</p>
<p>We report the following:</p>
<ul class="simple">
<li><p><strong>Accuracy</strong>: Proportion of correct predictions</p></li>
<li><p><strong>AUROC</strong>: Area under the Receiver Operating Characteristic curve, measuring ranking quality</p></li>
<li><p><strong>MCC</strong>: Matthews Correlation Coefficient, a balanced metric even for imbalanced classes</p></li>
</ul>
<p>The metrics are printed in a tabulated format using the <code class="docutils literal notranslate"><span class="pre">tabulate</span></code> library for clear presentation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tabulate</span><span class="w"> </span><span class="kn">import</span> <span class="n">tabulate</span>

<span class="c1"># Get validation metrics</span>
<span class="n">val_metrics</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">callback_metrics</span>
<span class="n">acc</span> <span class="o">=</span> <span class="n">val_metrics</span><span class="p">[</span><span class="s2">&quot;val_acc&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;val_acc&quot;</span> <span class="ow">in</span> <span class="n">val_metrics</span> <span class="k">else</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;nan&quot;</span><span class="p">)</span>
<span class="n">auc</span> <span class="o">=</span> <span class="n">val_metrics</span><span class="p">[</span><span class="s2">&quot;val_auroc&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;val_auroc&quot;</span> <span class="ow">in</span> <span class="n">val_metrics</span> <span class="k">else</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;nan&quot;</span><span class="p">)</span>
<span class="n">mcc</span> <span class="o">=</span> <span class="n">val_metrics</span><span class="p">[</span><span class="s2">&quot;val_mcc&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;val_mcc&quot;</span> <span class="ow">in</span> <span class="n">val_metrics</span> <span class="k">else</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;nan&quot;</span><span class="p">)</span>

<span class="c1"># Print metrics</span>
<span class="n">table_data</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="s2">&quot;Accuracy&quot;</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">acc</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">],</span>
    <span class="p">[</span><span class="s2">&quot;AUROC&quot;</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">auc</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">],</span>
    <span class="p">[</span><span class="s2">&quot;MCC&quot;</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">mcc</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">],</span>
<span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">=== Validation Summary ===&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tabulate</span><span class="p">(</span><span class="n">table_data</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Metric&quot;</span><span class="p">,</span> <span class="s2">&quot;Value&quot;</span><span class="p">],</span> <span class="n">tablefmt</span><span class="o">=</span><span class="s2">&quot;fancy_grid&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>=== Validation Summary ===
╒══════════╤═════════╕
│ Metric   │   Value │
╞══════════╪═════════╡
│ Accuracy │   0.663 │
├──────────┼─────────┤
│ AUROC    │   0.738 │
├──────────┼─────────┤
│ MCC      │   0.327 │
╘══════════╧═════════╛
</pre></div>
</div>
</div>
</div>
</section>
<section id="step-5-interpretation">
<h2>Step 5: Interpretation<a class="headerlink" href="#step-5-interpretation" title="Link to this heading">#</a></h2>
<p>We interpret the fine-tuned model using the multimodal_signal_image_attribution interpretation method from PyKale, which builds on Captum’s Integrated Gradients to generate visual explanations for both CXR and ECG inputs.
This method helps identify which regions in each modality, such as specific waveform segments in ECG and spatial regions in CXR, contributed most to the model’s prediction. This improves both transparency and clinical interpretability.</p>
<p><strong>Estimated runtime:</strong> 10 seconds</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">kale.interpret.signal_image_attribution</span><span class="w"> </span><span class="kn">import</span> <span class="n">multimodal_signal_image_attribution</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="c1"># User can change this to try different ECG and CXR interpretation configaration to play with</span>
<span class="n">cfg_FT</span><span class="o">.</span><span class="n">INTERPRET</span><span class="o">.</span><span class="n">ECG_THRESHOLD</span> <span class="o">=</span> <span class="mf">0.75</span>
<span class="n">cfg_FT</span><span class="o">.</span><span class="n">INTERPRET</span><span class="o">.</span><span class="n">SAMPLE_IDX</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">cfg_FT</span><span class="o">.</span><span class="n">INTERPRET</span><span class="o">.</span><span class="n">ZOOM_RANGE</span> <span class="o">=</span> <span class="p">[</span><span class="mf">3.5</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="n">cfg_FT</span><span class="o">.</span><span class="n">INTERPRET</span><span class="o">.</span><span class="n">CXR_THRESHOLD</span> <span class="o">=</span> <span class="mf">0.75</span>


<span class="n">sample_idx</span> <span class="o">=</span> <span class="n">cfg_FT</span><span class="o">.</span><span class="n">INTERPRET</span><span class="o">.</span><span class="n">SAMPLE_IDX</span>
<span class="n">zoom_range</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">cfg_FT</span><span class="o">.</span><span class="n">INTERPRET</span><span class="o">.</span><span class="n">ZOOM_RANGE</span><span class="p">)</span>
<span class="n">ecg_threshold</span> <span class="o">=</span> <span class="n">cfg_FT</span><span class="o">.</span><span class="n">INTERPRET</span><span class="o">.</span><span class="n">ECG_THRESHOLD</span>
<span class="n">cxr_threshold</span> <span class="o">=</span> <span class="n">cfg_FT</span><span class="o">.</span><span class="n">INTERPRET</span><span class="o">.</span><span class="n">CXR_THRESHOLD</span>
<span class="n">lead_number</span> <span class="o">=</span> <span class="n">cfg_FT</span><span class="o">.</span><span class="n">MODEL</span><span class="o">.</span><span class="n">NUM_LEADS</span>
<span class="n">sampling_rate</span> <span class="o">=</span> <span class="n">cfg_FT</span><span class="o">.</span><span class="n">INTERPRET</span><span class="o">.</span><span class="n">SAMPLING_RATE</span>


<span class="c1"># Run interpretation</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">multimodal_signal_image_attribution</span><span class="p">(</span>
    <span class="n">last_fold_model</span><span class="o">=</span><span class="n">model_pl</span><span class="p">,</span>
    <span class="n">last_val_loader</span><span class="o">=</span><span class="n">val_loader_FT</span><span class="p">,</span>
    <span class="n">sample_idx</span><span class="o">=</span><span class="n">sample_idx</span><span class="p">,</span>
    <span class="n">signal_threshold</span><span class="o">=</span><span class="n">ecg_threshold</span><span class="p">,</span>
    <span class="n">image_threshold</span><span class="o">=</span><span class="n">cxr_threshold</span><span class="p">,</span>
    <span class="n">zoom_range</span><span class="o">=</span><span class="n">zoom_range</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Full ECG View:</strong> Displays attribution across the full 12-lead ECG signal. A percentile-based threshold (e.g., top 25%) is applied to highlight segments with the highest contribution.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">patch</span><span class="o">.</span><span class="n">set_facecolor</span><span class="p">(</span><span class="s2">&quot;white&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_facecolor</span><span class="p">(</span><span class="s2">&quot;white&quot;</span><span class="p">)</span>

<span class="c1"># Plot the full ECG waveform</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">result</span><span class="p">[</span><span class="s2">&quot;full_time&quot;</span><span class="p">],</span>
    <span class="n">result</span><span class="p">[</span><span class="s2">&quot;signal_waveform_np&quot;</span><span class="p">][:</span> <span class="n">result</span><span class="p">[</span><span class="s2">&quot;full_length&quot;</span><span class="p">]],</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span>
    <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;ECG Waveform&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">result</span><span class="p">[</span><span class="s2">&quot;important_indices_full&quot;</span><span class="p">]:</span>
    <span class="n">stretch_start</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">idx</span> <span class="o">-</span> <span class="mi">6</span><span class="p">)</span>
    <span class="n">stretch_end</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="s2">&quot;full_length&quot;</span><span class="p">],</span> <span class="n">idx</span> <span class="o">+</span> <span class="mi">6</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
        <span class="n">result</span><span class="p">[</span><span class="s2">&quot;full_time&quot;</span><span class="p">][</span><span class="n">stretch_start</span><span class="p">:</span><span class="n">stretch_end</span><span class="p">],</span>
        <span class="n">result</span><span class="p">[</span><span class="s2">&quot;signal_waveform_np&quot;</span><span class="p">][</span><span class="n">stretch_start</span><span class="p">:</span><span class="n">stretch_end</span><span class="p">],</span>
        <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span>
        <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Time (seconds)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="s2">&quot;x-large&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Amplitude&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="s2">&quot;x-large&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Full ECG with Important Regions&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="s2">&quot;x-large&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">labelsize</span><span class="o">=</span><span class="s2">&quot;large&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">labelsize</span><span class="o">=</span><span class="s2">&quot;large&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;ECG Waveform&quot;</span><span class="p">,</span> <span class="s2">&quot;Important Regions&quot;</span><span class="p">],</span> <span class="n">fontsize</span><span class="o">=</span><span class="s2">&quot;medium&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/c619bafa45575f92b8862ad3b9c6e40cfb2c35961c483145777085300879f73b.png" src="../../_images/c619bafa45575f92b8862ad3b9c6e40cfb2c35961c483145777085300879f73b.png" />
</div>
</div>
<p><strong>Zoomed-In ECG Segment:</strong> Focuses on a specific time window (e.g., 3 to 4 seconds) for fine-grained inspection of high-attribution waveform regions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">patch</span><span class="o">.</span><span class="n">set_facecolor</span><span class="p">(</span><span class="s2">&quot;white&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_facecolor</span><span class="p">(</span><span class="s2">&quot;white&quot;</span><span class="p">)</span>

<span class="c1"># Plot zoomed-in ECG segment</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">result</span><span class="p">[</span><span class="s2">&quot;zoom_time&quot;</span><span class="p">],</span>
    <span class="n">result</span><span class="p">[</span><span class="s2">&quot;segment_signal_waveform&quot;</span><span class="p">],</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span>
    <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;ECG Waveform&quot;</span><span class="p">,</span>
<span class="p">)</span>


<span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">result</span><span class="p">[</span><span class="s2">&quot;important_indices_zoom&quot;</span><span class="p">]:</span>
    <span class="n">stretch_start</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">idx</span> <span class="o">-</span> <span class="mi">6</span><span class="p">)</span>
    <span class="n">stretch_end</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="s2">&quot;segment_signal_waveform&quot;</span><span class="p">]),</span> <span class="n">idx</span> <span class="o">+</span> <span class="mi">6</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
        <span class="n">result</span><span class="p">[</span><span class="s2">&quot;zoom_time&quot;</span><span class="p">][</span><span class="n">stretch_start</span><span class="p">:</span><span class="n">stretch_end</span><span class="p">],</span>
        <span class="n">result</span><span class="p">[</span><span class="s2">&quot;segment_signal_waveform&quot;</span><span class="p">][</span><span class="n">stretch_start</span><span class="p">:</span><span class="n">stretch_end</span><span class="p">],</span>
        <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span>
        <span class="n">linewidth</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
    <span class="p">)</span>


<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="s2">&quot;zoom_start_sec&quot;</span><span class="p">],</span> <span class="n">result</span><span class="p">[</span><span class="s2">&quot;zoom_end_sec&quot;</span><span class="p">],</span> <span class="mi">11</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="n">result</span><span class="p">[</span><span class="s2">&quot;zoom_start_sec&quot;</span><span class="p">],</span> <span class="n">result</span><span class="p">[</span><span class="s2">&quot;zoom_end_sec&quot;</span><span class="p">]])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Time (seconds)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="s2">&quot;x-large&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span>
    <span class="sa">f</span><span class="s1">&#39;Zoomed-In ECG Segment (</span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s2">&quot;zoom_start_sec&quot;</span><span class="p">]</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">s – </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s2">&quot;zoom_end_sec&quot;</span><span class="p">]</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">s)&#39;</span><span class="p">,</span>
    <span class="n">fontsize</span><span class="o">=</span><span class="s2">&quot;x-large&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/a7ac3b8c739d6cd265f297dc2fe00a7e01d31e333b84c6d9b44d9b6869581893.png" src="../../_images/a7ac3b8c739d6cd265f297dc2fe00a7e01d31e333b84c6d9b44d9b6869581893.png" />
</div>
</div>
<p><strong>CXR Attribution Map:</strong> Shows a heatmap over the input CXR image, with highlighted areas corresponding to regions above a configurable percentile threshold (e.g., top 25%). This helps reveal where the model focused attention in the spatial domain.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x_pts</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="s2">&quot;x_pts&quot;</span><span class="p">]</span>
<span class="n">y_pts</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="s2">&quot;y_pts&quot;</span><span class="p">]</span>
<span class="n">importance_pts</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="s2">&quot;importance_pts&quot;</span><span class="p">]</span>
<span class="n">cxr_thresh</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="s2">&quot;image_threshold&quot;</span><span class="p">]</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">patch</span><span class="o">.</span><span class="n">set_facecolor</span><span class="p">(</span><span class="s2">&quot;white&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_facecolor</span><span class="p">(</span><span class="s2">&quot;white&quot;</span><span class="p">)</span>

<span class="c1"># Show base CXR image</span>
<span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="s2">&quot;image_np&quot;</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Overlay attribution points</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">x_pts</span><span class="p">,</span>
    <span class="n">y_pts</span><span class="p">,</span>
    <span class="n">c</span><span class="o">=</span><span class="n">importance_pts</span><span class="p">,</span>
    <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;summer&quot;</span><span class="p">,</span>
    <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">,</span>
    <span class="n">s</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span>
    <span class="n">vmin</span><span class="o">=</span><span class="n">cxr_thresh</span><span class="p">,</span>
    <span class="n">vmax</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
    <span class="n">linewidths</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
    <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Add colorbar for importance</span>
<span class="n">cbar</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">fraction</span><span class="o">=</span><span class="mf">0.04</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mf">0.04</span><span class="p">)</span>
<span class="n">cbar</span><span class="o">.</span><span class="n">set_label</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Importance (</span><span class="si">{</span><span class="n">cxr_thresh</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">–1.00)&quot;</span><span class="p">)</span>

<span class="c1"># Final formatting</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;CXR: Important Regions&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="s2">&quot;x-large&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/8c52a46cde28bb1ba7b8f93c875c0b84458471a2f2617eedea05c819ea214d6c.png" src="../../_images/8c52a46cde28bb1ba7b8f93c875c0b84458471a2f2617eedea05c819ea214d6c.png" />
</div>
</div>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<p>[1] Lu, H., Liu, X., Zhou, S., Turner, R., Bai, P., Koot, R. E., … &amp; Xu, H. (2022, October). PyKale: Knowledge-aware machine learning from multiple sources in Python. In <em>Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management</em> (pp. 4274-4278).</p>
<p>[2] Johnson, A., Pollard, T., Mark, R., Berkowitz, S., &amp; Horng, S. (2024). MIMIC-CXR Database (version 2.1.0). <em>PhysioNet</em>. RRID:SCR_007345.</p>
<p>[3] Johnson, A. E., Pollard, T. J., Berkowitz, S. J., Greenbaum, N. R., Lungren, M. P., Deng, C. Y., … &amp; Horng, S. (2019). MIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text reports. <em>Scientific data</em>, 6(1), 317.</p>
<p>[4] Gow, B., Pollard, T., Nathanson, L. A., Johnson, A., Moody, B., Fernandes, C., Greenbaum, N., Waks, J. W., Eslami, P., Carbonati, T., Chaudhari, A., Herbst, E., Moukheiber, D., Berkowitz, S., Mark, R., &amp; Horng, S. (2023). MIMIC-IV-ECG: Diagnostic Electrocardiogram Matched Subset (version 1.0). PhysioNet. RRID:SCR_007345.</p>
<p>[5] Suvon, M. N., Tripathi, P. C., Fan, W., Zhou, S., Liu, X., Alabed, S., … &amp; Lu, H. (2024, October). Multimodal variational autoencoder for low-cost cardiac hemodynamics instability detection. In <em>International Conference on Medical Image Computing and Computer-Assisted Intervention</em> (pp. 296-306).</p>
</section>
<div class="toctree-wrapper compound">
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./tutorials/cardiac-hemodynamics-assessment"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../brain-disorder-diagnosis/extend-reading/helper-functions.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Helper Functions</p>
      </div>
    </a>
    <a class="right-next"
       href="extend-reading/extension-tasks.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Extension Tasks</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-0-environment-preparation">Step 0: Environment Preparation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#setup-google-colab-runtime-type">Setup Google Colab runtime type</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#package-installation">Package installation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#setup">Setup</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#configuration">Configuration</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pre-training-configuration">Pre-training Configuration</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuning-configuration">Fine-tuning Configuration</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-data-loading-and-preparation">Step 1: Data Loading and Preparation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pre-training-data-loading">Pre-training Data Loading</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuning-data-loading">Fine-tuning Data Loading</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-model-definition">Step 2: Model Definition</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#embed">Embed</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#signal-encoder">Signal Encoder</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#image-encoder">Image Encoder</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-fusion">Feature Fusion</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predict">Predict</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#reconstruction-pre-training">Reconstruction (Pre-training)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-fine-tuning">Classification (Fine-tuning)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-model-training">Step 3: Model Training</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multimodal-pretraining">Multimodal Pretraining</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multimodal-fine-tuning">Multimodal Fine-tuning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-evaluation">Step 4: Evaluation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-interpretation">Step 5: Interpretation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By PyKale Contributors
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>

<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Cardiothoracic Abnormality Assessment &#8212; PyKale</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/exercise.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'tutorials/cardiac-hemodynamics-assesment/tutorial';</script>
    <link rel="icon" href="../../_static/icon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/embc_logo.png" class="logo__image only-light" alt="PyKale - Home"/>
    <script>document.write(`<img src="../../_static/embc_logo.png" class="logo__image only-dark" alt="PyKale - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../drug-target-interaction/notebook-cross-domain.html">Drug‚ÄìTarget Interaction Prediction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multiomics-cancer-classification/notebook.html">Multiomics Cancer Classification</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/pykale/embc-mmai25/main?urlpath=tree/tutorials/cardiac-hemodynamics-assesment/tutorial.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Binder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Binder logo" src="../../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li><a href="https://colab.research.google.com/github/pykale/embc-mmai25/blob/main/tutorials/cardiac-hemodynamics-assesment/tutorial.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/pykale/embc-mmai25" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/pykale/embc-mmai25/issues/new?title=Issue%20on%20page%20%2Ftutorials/cardiac-hemodynamics-assesment/tutorial.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/tutorials/cardiac-hemodynamics-assesment/tutorial.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Cardiothoracic Abnormality Assessment</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-formulation">üìå Problem Formulation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#objectives">üéØ Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#environment-preparation">‚öôÔ∏è Environment preparation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#package-installation">Package installation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#setup">Setup</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#configuration">Configuration</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pre-training-configuration">Pre-training Configuration</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuning-configuration">Fine-tuning Configuration</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-preparation">üßπ Data Preparation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pre-training-data-loading">Pre-training Data Loading</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuning-data-loading">Fine-tuning Data Loading</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-definition">üß† Model Definition</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#embed">üì• Embed</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#signal-encoder">Signal Encoder</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#image-encoder">Image Encoder</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-fusion">Ô∏è Feature Fusion</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predict">üîÆ Predict</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#reconstruction-pre-training">Reconstruction (Pre-training)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-fine-tuning">Classification (Fine-tuning)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-training">üî• Model training</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multimodal-pretraining">üõ†Ô∏è Multimodal Pretraining</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multimodal-fine-tuning">üéØ Multimodal Fine-tuning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluate">üìà Evaluate</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-interpretation">üîç Model Interpretation</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="cardiothoracic-abnormality-assessment">
<h1>Cardiothoracic Abnormality Assessment<a class="headerlink" href="#cardiothoracic-abnormality-assessment" title="Link to this heading">#</a></h1>
<p><img alt="" src="../../_images/embc_heart_tutorial_fig.png" /></p>
<p>In this tutorial, we demonstrate how to use low-cost, non-invasive modalities <strong>Chest X-ray (CXR)</strong> and <strong>12-lead Electrocardiogram (ECG)</strong> to assess <strong>Cardiothoracic Abnormalities</strong>.</p>
<p><strong>‚è±Ô∏è Estimated runtime:</strong> Completing the steps in this tutorial will take approximately 8 minutes.</p>
<section id="problem-formulation">
<h2>üìå Problem Formulation<a class="headerlink" href="#problem-formulation" title="Link to this heading">#</a></h2>
<p>We will use a multimodal dataset derived from MIMIC-CXR and MIMIC-IV-ECG, which contains approximately 50K paired CXR and ECG samples. In this tutorial, we pretrain a multimodal <strong>CardioVAE</strong> model using ~49K CXR-ECG pairs via a tri-stream pretraining method. We then fine-tune this pretrained CardioVAE model on a smaller subset (~1K paired samples) with binary labels: <strong>Healthy</strong> and <strong>Cardiothoracic Abnormality</strong>. Lastly, we demonstrate how to interpret the trained CardioVAE model on both the CXR and ECG modalities.</p>
<p>This notebook is based on the work of <strong>Suvon et al. (MICCAI 2024)</strong>, which introduced a tri-stream pretraining strategy using a <strong>Multimodal Variational Autoencoder (VAE)</strong> to learn both modality-shared and modality-specific representations for assessing <strong>Pulmonary Arterial Wedge Pressure (PAWP)</strong>‚Äîa critical indicator of cardiac hemodynamics. The resulting model, <strong>CardioVAE</strong>, is implemented in the <a class="reference external" href="https://github.com/pykale/pykale">PyKale</a> library. Here, we provide a concise example of how to use this model through PyKale‚Äôs APIs‚Äîfrom pretraining and fine-tuning to model interpretation.</p>
</section>
<section id="objectives">
<h2>üéØ Objectives<a class="headerlink" href="#objectives" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Understand the roles of CXR and ECG in evaluating cardiac and thoracic health, and the benefits of multimodal modeling with these low-cost modalities.</p></li>
<li><p>Learn the standard PyKale workflow for pretraining, fine-tuning, and interpreting the CardioVAE model.</p></li>
</ol>
</section>
<section id="environment-preparation">
<h2>‚öôÔ∏è Environment preparation<a class="headerlink" href="#environment-preparation" title="Link to this heading">#</a></h2>
<section id="package-installation">
<h3>Package installation<a class="headerlink" href="#package-installation" title="Link to this heading">#</a></h3>
<p>The main packages required for this tutorial are:</p>
<ul class="simple">
<li><p><strong>pykale</strong>: An open-source machine learning library developed at the University of Sheffield, focused on biomedical and scientific applications. It supports multimodal learning, domain adaptation, and interpretability.</p></li>
<li><p><strong>wfdb</strong>: A toolkit for reading, writing, and processing physiological signal data, especially useful for ECG waveform analysis.</p></li>
<li><p><strong>yacs</strong>: A lightweight configuration management library that helps organize experimental settings in a structured, readable format.</p></li>
<li><p><strong>pytorch-lightning</strong>: A high-level framework built on PyTorch that simplifies training workflows, making code cleaner and easier to scale.</p></li>
<li><p><strong>tabulate</strong>: Used to print tabular data in a readable format, helpful for summarizing results or configuration parameters.</p></li>
</ul>
<p><strong>Additional Notes for Colab</strong>
Some non-critical dependencies (e.g., <code class="docutils literal notranslate"><span class="pre">torch-geometric</span></code>) may face version conflicts when installing <code class="docutils literal notranslate"><span class="pre">pykale</span></code> on Colab. These are handled manually in the below installation step. Also an autometic crash and reset has been added as after installing this depedencies you need to restart the session. <strong>Don‚Äôt run this block again if you already did once</strong></p>
<p><strong>‚è±Ô∏è Estimated runtime:</strong> 4 minutes</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import os

!pip uninstall --quiet -y torch torchvision torchaudio torchdata

!pip install --quiet torch==2.3.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-2.3.0+cu121.html

!pip install --quiet --user \
    git+https://github.com/pykale/pykale@main \
    yacs==0.1.8 wfdb pytorch-lightning tabulate captum neurokit2\
    &amp;&amp; echo &quot;pykale,yacs and wfdb installed successfully ‚úÖ&quot; \
    || echo &quot;Failed to install pykale,yacs and wfdb ‚ùå&quot;

# This code crashes the Colab runtime, triggering an automatic reset.
os.kill(os.getpid(), 9)
</pre></div>
</div>
</div>
</div>
</section>
<section id="setup">
<h3>Setup<a class="headerlink" href="#setup" title="Link to this heading">#</a></h3>
<p>As a starting point, we will mount Google Drive in Colab so that the data can be accessed directly. You might be prompted to grant permission to access your Google account‚Äîplease proceed with the authorisation when asked.</p>
<p>Next, we will install the required packages and load a set of helper functions to support the tutorial workflow. To keep the output clean and focused on interpretation, we also suppress unnecessary warnings.</p>
<p><strong>‚è±Ô∏è Estimated runtime:</strong> 25 seconds</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Connect with your google drive for data and model loading
from google.colab import drive

drive.mount(&quot;/content/drive&quot;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import os
import site
import sys
import warnings
import logging


# Disable warnings
warnings.filterwarnings(&quot;ignore&quot;)
os.environ[&quot;PYTHONWARNINGS&quot;] = &quot;ignore&quot;

# Suppress PyTorch Lightning logs
logging.getLogger(&quot;pytorch_lightning&quot;).setLevel(logging.ERROR)
logging.getLogger(&quot;pytorch_lightning.utilities.rank_zero&quot;).setLevel(logging.ERROR)
logging.getLogger(&quot;pytorch_lightning.accelerators.cuda&quot;).setLevel(logging.ERROR)


if &quot;google.colab&quot; in str(get_ipython()):
    sys.path.insert(0, site.getusersitepackages())
    !git clone --single-branch https://github.com/pykale/embc-mmai25.git
    %cp -r /content/embc-mmai25/tutorials/cardiac-hemodynamics-assesment/* /content/
    %rm -r /content/embc-mmai25
</pre></div>
</div>
</div>
</div>
</section>
<section id="configuration">
<h3>Configuration<a class="headerlink" href="#configuration" title="Link to this heading">#</a></h3>
<p>To maintain a clean and modular notebook design, <strong>CardioVAE</strong> uses dedicated configuration files for both pre-training and fine-tuning. This setup ensures a clear separation between code and experimental settings, enhancing reproducibility and flexibility across different tasks and datasets.</p>
<p>Configuration parameters can be overridden using external YAML files (e.g., <code class="docutils literal notranslate"><span class="pre">experiments/pretraining_base.yml</span></code>, <code class="docutils literal notranslate"><span class="pre">experiments/finetune_base.yml</span></code>).</p>
<section id="pre-training-configuration">
<h4>Pre-training Configuration<a class="headerlink" href="#pre-training-configuration" title="Link to this heading">#</a></h4>
<p>Default settings for the pre-training stage are defined in <code class="docutils literal notranslate"><span class="pre">config_pretrain.py</span></code>. These include data paths, model architecture, and optimizer parameters.
This modular structure allows easy experiment tracking and customisation by simply editing the associated <code class="docutils literal notranslate"><span class="pre">.yml</span></code> file.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>from config_pretrain import get_cfg_defaults

cfg_PT = get_cfg_defaults()
cfg_PT.merge_from_file(&quot;configs/pretraining_base.yml&quot;)

# ------ Hyperparameters to play with -----
cfg_PT.MODEL.LATENT_DIM = 128
cfg_PT.TRAIN.EPOCHS = 1
cfg_PT.TRAIN.LAMBDA_IMAGE = 1
cfg_PT.TRAIN.LAMBDA_SIGNAL = 10
# User can change this to try different batch size.
cfg_PT.DATA.BATCH_SIZE = 128

print(cfg_PT)
</pre></div>
</div>
</div>
</div>
</section>
<section id="fine-tuning-configuration">
<h4>Fine-tuning Configuration<a class="headerlink" href="#fine-tuning-configuration" title="Link to this heading">#</a></h4>
<p>Fine-tuning parameters are managed in <code class="docutils literal notranslate"><span class="pre">config_finetune.py</span></code>. These include learning rate, loss weights, number of epochs, model checkpoint paths, and other task-specific options.
External YAML files like <code class="docutils literal notranslate"><span class="pre">experiments/finetune_base.yml</span></code> enable flexible adjustments for different downstream tasks or datasets.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>from config_finetune import get_cfg_defaults

cfg_FT = get_cfg_defaults()
cfg_FT.merge_from_file(&quot;configs/finetune_base.yml&quot;)

# ------ Hyperparameters to play with -----
cfg_FT.FT.EPOCHS = 10
cfg_FT.FT.HIDDEN_DIM = 128
# User can change this to try different batch size.
cfg_FT.DATA.BATCH_SIZE = 32

print(cfg_FT)
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="data-preparation">
<h2>üßπ Data Preparation<a class="headerlink" href="#data-preparation" title="Link to this heading">#</a></h2>
<p>This tutorial uses separate data pipelines for <strong>pre-training</strong> and <strong>fine-tuning</strong>, both based on paired chest X-ray (CXR) and ECG signal features. Each stage follows standard preprocessing steps‚Äîsuch as resizing, normalization, interpolation, and tensor conversion‚Äîtailored for resource-constrained environments like <strong>Google Colab</strong>.</p>
<p>PyKale API for Data preparation:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">kale.loaddata.signal_access.load_ecg_from_folder</span></code> provides a convenient function for loading ECG waveform data stored in a directory structure. It supports automatic parsing and conversion of ECG signal files into PyTorch tensors, with options for standard preprocessing such as normalisation, resize, interpolation, and resampling. This enables streamlined integration with deep learning pipelines.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">kale.loaddata.image_access.load_images_from_dir</span></code> offers an easy-to-use utility for loading image datasets from directory hierarchies. It supports standard image formats and returns PyTorch tensors, performing essential preprocessing steps such as resizing and normalisation. This function is suitable for image classification, computer vision, and multimodal learning tasks.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">kale.loaddata.signal_image_access.SignalImageDataset</span></code> defines a unified dataset class designed for paired signal (e.g., ECG) and image (e.g., CXR) modalities. It facilitates synchronized access to multi-source data, providing ready-to-use PyTorch datasets that can be directly utilised for multimodal training, evaluation, and downstream applications.</p></li>
</ul>
<p><strong>Note:</strong> Please create a shortcut to the following Google Drive folder in your <strong>MyDrive</strong>:<br />
üëâ <a class="reference external" href="https://drive.google.com/drive/folders/1N7-fMWsdK-tuB76SdC-GF1njYYGx0Z-i?usp=sharing">Google Drive Link</a></p>
<p>There‚Äôs no need to download the data manually. After mounting your Google Drive in the setup section, you will be able to directly access all datasets and pretrained models.</p>
<section id="pre-training-data-loading">
<h3>Pre-training Data Loading<a class="headerlink" href="#pre-training-data-loading" title="Link to this heading">#</a></h3>
<p>To accommodate the resource constraints of platforms like <strong>Google Colab</strong>, this tutorial uses a lightweight version of the dataset consisting of the <strong>first 1,000 preprocessed samples</strong> from the full 50K paired CXR and ECG dataset. This significantly reduces runtime and memory requirements, allowing for rapid experimentation without the overhead of full-scale data loading and transformation.</p>
<p>The complete preprocessing pipeline, implemented using the PyKale API, is provided for reference. Additionally, CSV files containing subject IDs for the full dataset are provided. Users interested in training on the complete 50K dataset can leverage the <strong>PyKale API</strong>, which supports direct loading of raw CXR and ECG features with integrated preprocessing.</p>
<p><strong>Note:</strong></p>
<ul class="simple">
<li><p>For ease of use in Colab, the full-data loading functionality is <strong>commented out by default</strong>. It can be re-enabled for local or high-resource environments.</p></li>
<li><p>To access the required files, ensure that the shared folder <strong><code class="docutils literal notranslate"><span class="pre">EMBC_workshop_data</span></code></strong> is added as a <strong>shortcut to your Google Drive (under ‚ÄúMy Drive‚Äù)</strong>.</p></li>
</ul>
<p><strong>‚è±Ô∏è Estimated runtime:</strong> 12 seconds</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># (OPTIONAL)
# from kale.loaddata.signal_access import load_ecg_from_folder
# from kale.loaddata.image_access import load_images_from_dir

# ecg_tensor = load_ecg_from_folder(&quot;/data/ecg/&quot;, &quot;ecg_files.csv&quot;)
# cxr_tensor = load_images_from_dir(&quot;/data/cxr/&quot;, &quot;cxr_files.csv&quot;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import torch
from kale.loaddata.signal_image_access import SignalImageDataset
from kale.utils.seed import set_seed
import random
import numpy as np
import torch

set_seed(cfg_PT.TRAIN.SEED)

ecg_tensor_PT = torch.load(cfg_PT.DATA.ECG_PATH, map_location=cfg_PT.TRAIN.DATA_DEVICE)
cxr_tensor_PT = torch.load(cfg_PT.DATA.CXR_PATH, map_location=cfg_PT.TRAIN.DATA_DEVICE)

train_dataset_PT, val_dataset_PT = SignalImageDataset.prepare_data_loaders(
    ecg_tensor_PT, cxr_tensor_PT
)
</pre></div>
</div>
</div>
</div>
</section>
<section id="fine-tuning-data-loading">
<h3>Fine-tuning Data Loading<a class="headerlink" href="#fine-tuning-data-loading" title="Link to this heading">#</a></h3>
<p>For the fine-tuning stage, we use the <strong>last 1,000 paired CXR and ECG samples</strong> from the full 50K dataset derived from <strong>MIMIC-CXR</strong> and <strong>MIMIC-IV-ECG</strong>. Corresponding disease labels are obtained from MIMIC-CXR, which includes 12 cardiothoracic abnormality classes along with a ‚ÄúNo Finding‚Äù label representing healthy cases.</p>
<p>To formulate a binary classification task, all abnormality classes are grouped into a single <strong>Cardiothoracic Abnormality</strong> category, while the ‚ÄúNo Finding‚Äù label is treated as <strong>Healthy</strong>. The resulting label mapping is as follows:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">0</span></code> ‚Üí <strong>Healthy</strong></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">1</span></code> ‚Üí <strong>Cardiothoracic Abnormality</strong></p></li>
</ul>
<p>This fine-tuning subset is explicitly selected to ensure no overlap with the samples used during pre-training, thereby simulating a realistic downstream evaluation setting.</p>
<p>Unlike the fine-tuning strategy reported in <em>Suvon et al., MICCAI 2024</em>, which relied on a private in-house dataset, this approach is fully reproducible using publicly available data from MIMIC-CXR and MIMIC-IV-ECG.</p>
<p><strong>‚è±Ô∏è Estimated runtime:</strong> 10 seconds</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import torch
from torch.utils.data import TensorDataset, DataLoader, random_split
import pandas as pd

# Set seed for reproducibility
torch.manual_seed(cfg_FT.FT.SEED)

# Load data
ecg_tensor_FT = torch.load(cfg_FT.DATA.ECG_PATH, map_location=cfg_FT.DATA.DATA_DEVICE)
cxr_tensor_FT = torch.load(cfg_FT.DATA.CXR_PATH, map_location=cfg_FT.DATA.DATA_DEVICE)
label_df = pd.read_csv(cfg_FT.DATA.CSV_PATH)
labels = torch.tensor(label_df[&quot;label&quot;].values, dtype=torch.long)

# Combine tensors into a single dataset
dataset = TensorDataset(cxr_tensor_FT, ecg_tensor_FT, labels)

# Split into train/val
val_ratio = 0.2
num_samples = len(dataset)
num_val = int(val_ratio * num_samples)
num_train = num_samples - num_val

train_dataset_FT, val_dataset_FT = random_split(
    dataset,
    [num_train, num_val],
    generator=torch.Generator().manual_seed(cfg_FT.FT.SEED),
)

# DataLoaders
train_loader_FT = DataLoader(
    train_dataset_FT,
    batch_size=cfg_FT.DATA.BATCH_SIZE,
    shuffle=True,
    num_workers=cfg_FT.DATA.NUM_WORKERS,
)
val_loader_FT = DataLoader(
    val_dataset_FT,
    batch_size=cfg_FT.DATA.BATCH_SIZE,
    shuffle=False,
    num_workers=cfg_FT.DATA.NUM_WORKERS,
)
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="model-definition">
<h2>üß† Model Definition<a class="headerlink" href="#model-definition" title="Link to this heading">#</a></h2>
<p>We use the <strong>PyKale</strong> library to implement a modular multimodal variational autoencoder (VAE) for learning joint representations from <strong>ECG</strong> and <strong>CXR</strong> data. The architecture includes modality-specific encoders and decoders, a PoE-based fusion mechanism, and task-specific heads for reconstruction and classification.</p>
<section id="embed">
<h3>üì• Embed<a class="headerlink" href="#embed" title="Link to this heading">#</a></h3>
<p>The embedding module is composed of independent encoders for each modality and a fusion mechanism to obtain a shared latent representation.</p>
<section id="signal-encoder">
<h4>Signal Encoder<a class="headerlink" href="#signal-encoder" title="Link to this heading">#</a></h4>
<p>The ECG signal pathway uses <code class="docutils literal notranslate"><span class="pre">SignalVAEEncoder</span></code> from <code class="docutils literal notranslate"><span class="pre">kale.embed.signal_cnn</span></code>.<br />
This encoder captures high-level temporal features from preprocessed ECG waveforms and maps them to a latent space suitable for downstream fusion and representation learning.</p>
</section>
<section id="image-encoder">
<h4>Image Encoder<a class="headerlink" href="#image-encoder" title="Link to this heading">#</a></h4>
<p>The image pathway uses <code class="docutils literal notranslate"><span class="pre">ImageVAEEncoder</span></code> from <code class="docutils literal notranslate"><span class="pre">kale.embed.image_cnn</span></code>.<br />
This encoder captures high-level spatial features from preprocessed CXR‚Äôs and maps them to a latent space suitable for downstream fusion and representation learning.</p>
</section>
<section id="feature-fusion">
<h4>Ô∏è Feature Fusion<a class="headerlink" href="#feature-fusion" title="Link to this heading">#</a></h4>
<p>Encoded modality-specific features are combined using a <strong>Product-of-Experts (PoE)</strong> approach, implemented in <code class="docutils literal notranslate"><span class="pre">kale.embed.feature_fusion</span></code>.<br />
The PoE fusion computes a joint posterior over the latent space by aggregating information from each modality, enabling coherent multimodal representation.</p>
</section>
</section>
<hr class="docutils" />
<section id="predict">
<h3>üîÆ Predict<a class="headerlink" href="#predict" title="Link to this heading">#</a></h3>
<section id="reconstruction-pre-training">
<h4>Reconstruction (Pre-training)<a class="headerlink" href="#reconstruction-pre-training" title="Link to this heading">#</a></h4>
<p>During pre-training, the model reconstructs both modalities using decoders from the shared latent representation:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">ImageVAEDecoder</span></code> from <code class="docutils literal notranslate"><span class="pre">kale.embed.vae_decoder</span></code> for CXR reconstruction</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">SignalVAEDecoder</span></code> from <code class="docutils literal notranslate"><span class="pre">kale.embed.signal_cnn_vae</span></code> for ECG waveform reconstruction</p></li>
</ul>
<p>The model is trained to minimise the <strong>Evidence Lower Bound (ELBO)</strong>, encouraging informative and disentangled latent representations.</p>
</section>
<section id="classification-fine-tuning">
<h4>Classification (Fine-tuning)<a class="headerlink" href="#classification-fine-tuning" title="Link to this heading">#</a></h4>
<p>For downstream classification tasks, we reuse the pretrained encoders as feature extractors.<br />
The <code class="docutils literal notranslate"><span class="pre">SignalImageFineTuningClassifier</span></code> from <a class="reference external" href="https://github.com/pykale/pykale/blob/main/kale/pipeline/finetune.py"><code class="docutils literal notranslate"><span class="pre">kale.pipeline.finetune</span></code></a> adds a lightweight classification head on top of the shared latent space for supervised learning.<br />
This setup is optimised for clinical prediction tasks, such as binary or multi-label disease classification.</p>
</section>
</section>
</section>
<section id="model-training">
<h2>üî• Model training<a class="headerlink" href="#model-training" title="Link to this heading">#</a></h2>
<section id="multimodal-pretraining">
<h3>üõ†Ô∏è Multimodal Pretraining<a class="headerlink" href="#multimodal-pretraining" title="Link to this heading">#</a></h3>
<p>We pretraind a CardioVAE model using the <code class="docutils literal notranslate"><span class="pre">SignalImageVAE</span></code> class from <strong>PyKale</strong> to jointly model paired CXR and ECG data. The goal is to learn <strong>shared and modality-specific representations</strong> in an <strong>unsupervised</strong> manner via reconstruction.</p>
<p>We instantiate <code class="docutils literal notranslate"><span class="pre">SignalImageVAE</span></code> from <a class="reference external" href="https://github.com/pykale/pykale/blob/main/kale/embed/multimodal_encoder.py"><code class="docutils literal notranslate"><span class="pre">kale.embed.multimodal_encoder</span></code></a>, which includes:</p>
<ul class="simple">
<li><p>A <strong>signal encoder-decoder</strong> built on <code class="docutils literal notranslate"><span class="pre">SignalVAEEncoder</span></code> for ECG waveforms</p></li>
<li><p>An <strong>image encoder-decoder</strong> built on <code class="docutils literal notranslate"><span class="pre">ImageVAEEncoder</span></code> for CXR images</p></li>
<li><p>A <strong>Product-of-Experts (PoE)</strong> fusion module for combining modality-specific latent vectors into a shared latent representation</p></li>
</ul>
<p>To pretrain, we use <code class="docutils literal notranslate"><span class="pre">SignalImageTriStreamVAETrainer</span></code> from <a class="reference external" href="https://github.com/pykale/pykale/blob/main/kale/pipeline/multimodal_trainer.py"><code class="docutils literal notranslate"><span class="pre">kale.pipeline.multimodal_trainer</span></code></a> to:</p>
<ul class="simple">
<li><p>Perform <strong>joint and single-modality reconstructions</strong></p></li>
<li><p>Optimise the <strong>ELBO loss</strong>, balancing image and signal modalities</p></li>
<li><p>Manage logging, and reconstruction-based validation</p></li>
</ul>
<p><strong>‚è±Ô∏è Estimated runtime:</strong> 2 minutes with 1 epoch</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import pytorch_lightning as pl
from kale.pipeline.multimodal_trainer import SignalImageTriStreamVAETrainer
from kale.embed.multimodal_encoder import SignalImageVAE

model = SignalImageVAE(
    image_input_channels=cfg_PT.MODEL.INPUT_DIM_CXR,
    signal_input_dim=cfg_PT.MODEL.INPUT_DIM_ECG,
    latent_dim=cfg_PT.MODEL.LATENT_DIM,
)

# PyKale trainer instance (all from config)
pl_trainer = SignalImageTriStreamVAETrainer(
    model=model,
    train_dataset=train_dataset_PT,
    val_dataset=val_dataset_PT,
    batch_size=cfg_PT.DATA.BATCH_SIZE,
    num_workers=cfg_PT.DATA.NUM_WORKERS,
    lambda_image=cfg_PT.TRAIN.LAMBDA_IMAGE,
    lambda_signal=cfg_PT.TRAIN.LAMBDA_SIGNAL,
    lr=cfg_PT.TRAIN.LR,
    annealing_epochs=cfg_PT.TRAIN.EPOCHS,
    scale_factor=cfg_PT.TRAIN.SCALE_FACTOR,
)

trainer = pl.Trainer(
    max_epochs=cfg_PT.TRAIN.EPOCHS,
    accelerator=cfg_PT.TRAIN.ACCELERATOR,
    devices=cfg_PT.TRAIN.DEVICES,
    log_every_n_steps=10,
)

trainer.fit(pl_trainer)

# Save model state dict
torch.save(model.state_dict(), cfg_PT.TRAIN.SAVE_PATH)
print(f&quot;Saved model state dictionary to &#39;{cfg_PT.TRAIN.SAVE_PATH}&#39;&quot;)
</pre></div>
</div>
</div>
</div>
</section>
<section id="multimodal-fine-tuning">
<h3>üéØ Multimodal Fine-tuning<a class="headerlink" href="#multimodal-fine-tuning" title="Link to this heading">#</a></h3>
<p>For downstream classification, we fine-tune a shallow classifier on top of a <strong>pretrained CardioVAE encoder</strong>. The encoder is loaded from <code class="docutils literal notranslate"><span class="pre">SignalImageVAE</span></code>, pretrained with reconstruction loss, and used as a fixed or partially trainable <strong>feature extractor</strong>.</p>
<p>We use <code class="docutils literal notranslate"><span class="pre">SignalImageFineTuningClassifier</span></code> from <a class="reference external" href="https://github.com/pykale/pykale/blob/main/kale/pipeline/finetune.py"><code class="docutils literal notranslate"><span class="pre">kale.pipeline.finetune</span></code></a>, which wraps:</p>
<ul class="simple">
<li><p>A <strong>pretrained encoder</strong> from <code class="docutils literal notranslate"><span class="pre">SignalImageVAE</span></code></p></li>
<li><p>A <strong>classification head</strong> (single or multi-layer MLP)</p></li>
<li><p>A <strong>training step</strong> that supports standard supervised learning with cross-entropy loss</p></li>
</ul>
<p><strong>‚è±Ô∏è Estimated runtime:</strong> 1 minute with 10 epoch</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import torch
import pytorch_lightning as pl
from kale.embed.multimodal_encoder import SignalImageVAE
from kale.pipeline.multimodal_trainer import SignalImageFineTuningTrainer
from kale.utils.remap_model_parameters import remap_state_dict_keys

# Load and remap checkpoint
checkpoint = torch.load(cfg_FT.FT.CKPT_PATH, map_location=cfg_FT.FT.DEVICE)
checkpoint = remap_state_dict_keys(checkpoint)

pretrained_mvae = SignalImageVAE(
    image_input_channels=cfg_FT.MODEL.INPUT_IMAGE_CHANNELS,
    signal_input_dim=cfg_FT.MODEL.INPUT_DIM_ECG,
    latent_dim=cfg_FT.MODEL.LATENT_DIM,
)
pretrained_mvae.load_state_dict(checkpoint, strict=False)
pretrained_mvae.to(cfg_FT.FT.DEVICE)
pretrained_mvae.eval()

model_pl = SignalImageFineTuningTrainer(
    pretrained_model=pretrained_mvae,
    num_classes=cfg_FT.FT.NUM_CLASSES,
    lr=cfg_FT.FT.LR,
    hidden_dim=cfg_FT.FT.HIDDEN_DIM,
)

trainer = pl.Trainer(
    max_epochs=cfg_FT.FT.EPOCHS,
    accelerator=cfg_FT.FT.ACCELERATOR,
    devices=cfg_FT.FT.DEVICES,
    log_every_n_steps=10,
    enable_checkpointing=False,
    logger=False,
)

trainer.fit(model_pl, train_dataloaders=train_loader_FT, val_dataloaders=val_loader_FT)
</pre></div>
</div>
</div>
</div>
</section>
<section id="evaluate">
<h3>üìà Evaluate<a class="headerlink" href="#evaluate" title="Link to this heading">#</a></h3>
<p>After training, we extract key validation metrics using PyTorch Lightning‚Äôs built-in <code class="docutils literal notranslate"><span class="pre">callback_metrics</span></code>. These metrics provide a quantitative summary of model performance on the validation set.</p>
<p>We report the following:</p>
<ul class="simple">
<li><p><strong>Accuracy</strong>: Proportion of correct predictions</p></li>
<li><p><strong>AUROC</strong>: Area under the Receiver Operating Characteristic curve, measuring ranking quality</p></li>
<li><p><strong>MCC</strong>: Matthews Correlation Coefficient, a balanced metric even for imbalanced classes</p></li>
</ul>
<p>The metrics are printed in a tabulated format using the <code class="docutils literal notranslate"><span class="pre">tabulate</span></code> library for clear presentation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>from tabulate import tabulate

# Get validation metrics
val_metrics = trainer.callback_metrics
acc = val_metrics[&quot;val_acc&quot;].item() if &quot;val_acc&quot; in val_metrics else float(&quot;nan&quot;)
auc = val_metrics[&quot;val_auroc&quot;].item() if &quot;val_auroc&quot; in val_metrics else float(&quot;nan&quot;)
mcc = val_metrics[&quot;val_mcc&quot;].item() if &quot;val_mcc&quot; in val_metrics else float(&quot;nan&quot;)

# Print metrics
table_data = [
    [&quot;Accuracy&quot;, f&quot;{acc:.3f}&quot;],
    [&quot;AUROC&quot;, f&quot;{auc:.3f}&quot;],
    [&quot;MCC&quot;, f&quot;{mcc:.3f}&quot;],
]
print(&quot;\n=== Validation Summary ===&quot;)
print(tabulate(table_data, headers=[&quot;Metric&quot;, &quot;Value&quot;], tablefmt=&quot;fancy_grid&quot;))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>=== Validation Summary ===
‚ïí‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïï
‚îÇ Metric   ‚îÇ   Value ‚îÇ
‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°
‚îÇ Accuracy ‚îÇ   0.663 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ AUROC    ‚îÇ   0.726 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ MCC      ‚îÇ   0.326 ‚îÇ
‚ïò‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïõ
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="model-interpretation">
<h2>üîç Model Interpretation<a class="headerlink" href="#model-interpretation" title="Link to this heading">#</a></h2>
<p>We interpret the fine-tuned model using the multimodal_signal_image_attribution interpretation method from PyKale, which builds on Captum‚Äôs Integrated Gradients to generate visual explanations for both CXR and ECG inputs.
This method helps identify which regions in each modality, such as specific waveform segments in ECG and spatial regions in CXR, contributed most to the model‚Äôs prediction. This improves both transparency and clinical interpretability.</p>
<p><strong>‚è±Ô∏è Estimated runtime:</strong> 10 seconds</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>from kale.interpret.signal_image_attribution import multimodal_signal_image_attribution
import matplotlib.pyplot as plt
import numpy as np

# User can change this to try different ECG and CXR interpretation configaration to play with
cfg_FT.INTERPRET.ECG_THRESHOLD = 0.75
cfg_FT.INTERPRET.SAMPLE_IDX = 3
cfg_FT.INTERPRET.ZOOM_RANGE = [3.5, 4]
cfg_FT.INTERPRET.CXR_THRESHOLD = 0.75


sample_idx = cfg_FT.INTERPRET.SAMPLE_IDX
zoom_range = tuple(cfg_FT.INTERPRET.ZOOM_RANGE)
ecg_threshold = cfg_FT.INTERPRET.ECG_THRESHOLD
cxr_threshold = cfg_FT.INTERPRET.CXR_THRESHOLD
lead_number = cfg_FT.MODEL.NUM_LEADS
sampling_rate = cfg_FT.INTERPRET.SAMPLING_RATE


# Run interpretation
result = multimodal_signal_image_attribution(
    last_fold_model=model_pl,
    last_val_loader=val_loader_FT,
    sample_idx=sample_idx,
    signal_threshold=ecg_threshold,
    image_threshold=cxr_threshold,
    zoom_range=zoom_range,
)
</pre></div>
</div>
</div>
</div>
<p><strong>Full ECG View:</strong> Displays attribution across the full 12-lead ECG signal. A percentile-based threshold (e.g., top 25%) is applied to highlight segments with the highest contribution.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>fig, ax = plt.subplots(figsize=(12, 5))
fig.patch.set_facecolor(&quot;white&quot;)
ax.set_facecolor(&quot;white&quot;)

# Plot the full ECG waveform
ax.plot(
    result[&quot;full_time&quot;],
    result[&quot;signal_waveform_np&quot;][: result[&quot;full_length&quot;]],
    color=&quot;black&quot;,
    linewidth=1,
    label=&quot;ECG Waveform&quot;,
)

for idx in result[&quot;important_indices_full&quot;]:
    stretch_start = max(0, idx - 6)
    stretch_end = min(result[&quot;full_length&quot;], idx + 6 + 1)
    ax.plot(
        result[&quot;full_time&quot;][stretch_start:stretch_end],
        result[&quot;signal_waveform_np&quot;][stretch_start:stretch_end],
        color=&quot;red&quot;,
        linewidth=2,
    )

ax.set_xlabel(&quot;Time (seconds)&quot;, fontsize=&quot;x-large&quot;)
ax.set_ylabel(&quot;Amplitude&quot;, fontsize=&quot;x-large&quot;)
ax.set_title(&quot;Full ECG with Important Regions&quot;, fontsize=&quot;x-large&quot;)
ax.set_xticks(np.linspace(0, 10, 11))
ax.set_xlim([0, 10])
ax.tick_params(axis=&quot;x&quot;, labelsize=&quot;large&quot;)
ax.tick_params(axis=&quot;y&quot;, labelsize=&quot;large&quot;)
ax.legend([&quot;ECG Waveform&quot;, &quot;Important Regions&quot;], fontsize=&quot;medium&quot;)

plt.tight_layout()
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/4eceee9f036697bc7c320d8f448b1a507bfbf964620460029e87e3737a7e0f67.png" src="../../_images/4eceee9f036697bc7c320d8f448b1a507bfbf964620460029e87e3737a7e0f67.png" />
</div>
</div>
<p><strong>Zoomed-In ECG Segment:</strong> Focuses on a specific time window (e.g., 3 to 4 seconds) for fine-grained inspection of high-attribution waveform regions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>fig, ax = plt.subplots(figsize=(12, 5))
fig.patch.set_facecolor(&quot;white&quot;)
ax.set_facecolor(&quot;white&quot;)

# Plot zoomed-in ECG segment
ax.plot(
    result[&quot;zoom_time&quot;],
    result[&quot;segment_signal_waveform&quot;],
    color=&quot;black&quot;,
    linewidth=3,
    label=&quot;ECG Waveform&quot;,
)


for idx in result[&quot;important_indices_zoom&quot;]:
    stretch_start = max(0, idx - 6)
    stretch_end = min(len(result[&quot;segment_signal_waveform&quot;]), idx + 6 + 1)
    ax.plot(
        result[&quot;zoom_time&quot;][stretch_start:stretch_end],
        result[&quot;segment_signal_waveform&quot;][stretch_start:stretch_end],
        color=&quot;red&quot;,
        linewidth=6,
    )


ax.set_xticks(np.linspace(result[&quot;zoom_start_sec&quot;], result[&quot;zoom_end_sec&quot;], 11))
ax.set_xlim([result[&quot;zoom_start_sec&quot;], result[&quot;zoom_end_sec&quot;]])
ax.set_yticks([])
ax.set_xlabel(&quot;Time (seconds)&quot;, fontsize=&quot;x-large&quot;)
ax.set_ylabel(&quot;&quot;)
ax.set_title(
    f&#39;Zoomed-In ECG Segment ({result[&quot;zoom_start_sec&quot;]:.2f}s ‚Äì {result[&quot;zoom_end_sec&quot;]:.2f}s)&#39;,
    fontsize=&quot;x-large&quot;,
)

plt.tight_layout()
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/a7ac3b8c739d6cd265f297dc2fe00a7e01d31e333b84c6d9b44d9b6869581893.png" src="../../_images/a7ac3b8c739d6cd265f297dc2fe00a7e01d31e333b84c6d9b44d9b6869581893.png" />
</div>
</div>
<p><strong>CXR Attribution Map:</strong> Shows a heatmap over the input CXR image, with highlighted areas corresponding to regions above a configurable percentile threshold (e.g., top 25%). This helps reveal where the model focused attention in the spatial domain.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>x_pts = result[&quot;x_pts&quot;]
y_pts = result[&quot;y_pts&quot;]
importance_pts = result[&quot;importance_pts&quot;]
cxr_thresh = result[&quot;image_threshold&quot;]

fig, ax = plt.subplots(figsize=(6, 6))
fig.patch.set_facecolor(&quot;white&quot;)
ax.set_facecolor(&quot;white&quot;)

# Show base CXR image
ax.imshow(result[&quot;image_np&quot;], cmap=&quot;gray&quot;, alpha=1)

# Overlay attribution points
sc = ax.scatter(
    x_pts,
    y_pts,
    c=importance_pts,
    cmap=&quot;summer&quot;,
    marker=&quot;x&quot;,
    s=150,
    vmin=cxr_thresh,
    vmax=1.0,
    alpha=0.8,
    linewidths=0.7,
    edgecolor=&quot;black&quot;,
)

# Add colorbar for importance
cbar = plt.colorbar(sc, ax=ax, fraction=0.04, pad=0.04)
cbar.set_label(f&quot;Importance ({cxr_thresh:.2f}‚Äì1.00)&quot;)

# Final formatting
ax.axis(&quot;off&quot;)
ax.set_title(&quot;CXR: Important Regions&quot;, fontsize=&quot;x-large&quot;)

plt.tight_layout()
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/4303f2a108def85e2bd9ba6c24db8b949044c8a573e81468599331e71068b487.png" src="../../_images/4303f2a108def85e2bd9ba6c24db8b949044c8a573e81468599331e71068b487.png" />
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./tutorials/cardiac-hemodynamics-assesment"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-formulation">üìå Problem Formulation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#objectives">üéØ Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#environment-preparation">‚öôÔ∏è Environment preparation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#package-installation">Package installation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#setup">Setup</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#configuration">Configuration</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pre-training-configuration">Pre-training Configuration</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuning-configuration">Fine-tuning Configuration</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-preparation">üßπ Data Preparation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pre-training-data-loading">Pre-training Data Loading</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuning-data-loading">Fine-tuning Data Loading</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-definition">üß† Model Definition</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#embed">üì• Embed</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#signal-encoder">Signal Encoder</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#image-encoder">Image Encoder</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-fusion">Ô∏è Feature Fusion</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predict">üîÆ Predict</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#reconstruction-pre-training">Reconstruction (Pre-training)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-fine-tuning">Classification (Fine-tuning)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-training">üî• Model training</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multimodal-pretraining">üõ†Ô∏è Multimodal Pretraining</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multimodal-fine-tuning">üéØ Multimodal Fine-tuning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluate">üìà Evaluate</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-interpretation">üîç Model Interpretation</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By PyKale Contributors
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>